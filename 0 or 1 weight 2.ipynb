{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff569d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5494ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "unit = []\n",
    "single = []\n",
    "tripple = []\n",
    "for i in range(0,100000):\n",
    "    single.append(random.randint(-100, 100))\n",
    "    \n",
    "for i in range(0,100000):\n",
    "    temp1 = random.randint(-10, 10)\n",
    "    temp2 = random.randint(-10, 10)\n",
    "    temp3 = random.randint(-10, 10)\n",
    "    tripple.append([temp1, temp2, temp3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65199f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(tripple)):\n",
    "    tripple[i].append(single[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c300af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit = tripple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63aa961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "answer = []\n",
    "for l in unit:\n",
    "    temp = False\n",
    "    if(l[0]>20 and abs(l[1] < 3) and abs(l[2] < 3) and abs(l[3] < 3)):\n",
    "        temp = True\n",
    "    if(l[1]>20 and abs(l[2] < 3) and abs(l[3] < 3) and abs(l[0] < 3)):\n",
    "        temp = True\n",
    "    if(l[2]>20 and abs(l[3] < 3) and abs(l[0] < 3) and abs(l[1] < 3)):\n",
    "        temp = True\n",
    "    if(l[3]>20 and abs(l[1] < 3) and abs(l[2] < 3) and abs(l[0] < 3)):\n",
    "        temp = True\n",
    "    answer.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2157f80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9638"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for a in answer:\n",
    "    if(a==True):\n",
    "        count+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c00b7492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = tf.keras.utils.to_categorical(answer)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "080a2638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "labels = np.array(labels)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da5a89e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95803864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.array(unit)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f33bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0 = tf.keras.models.Sequential()\n",
    "m0.add(tf.keras.layers.Dense(2, activation = \"softmax\", input_shape=(4,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b1ea71a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 12)                60        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 52        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 122\n",
      "Trainable params: 122\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "db7cd307",
   "metadata": {},
   "outputs": [],
   "source": [
    "m0.compile(optimizer='rmsprop',\n",
    "                loss='mean_squared_error',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "47da30f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0445 - accuracy: 0.9381\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 1s 738us/step - loss: 0.0445 - accuracy: 0.9382\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 1s 697us/step - loss: 0.0445 - accuracy: 0.9380\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 1s 776us/step - loss: 0.0445 - accuracy: 0.9380\n",
      "Epoch 5/10\n",
      " 782/1000 [======================>.......] - ETA: 0s - loss: 0.0445 - accuracy: 0.9380"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-342-51ea428cd1c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "m0.fit(inputs,labels,epochs = 10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "956c9cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.36966568,  0.12533066],\n",
       "        [-0.6448173 , -0.89067745],\n",
       "        [ 0.05684882, -0.17444837],\n",
       "        [ 0.43259695,  0.47390565]], dtype=float32),\n",
       " array([ 2.6688612, -2.6688607], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m0.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "1ecb6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = tf.keras.models.Sequential()\n",
    "m3.add(tf.keras.layers.Dense(4, activation = \"relu\", input_shape=(4,)))\n",
    "m3.add(tf.keras.layers.Dense(2, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "b5468aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = tf.keras.models.Sequential()\n",
    "for layer in m3.layers[:-1]:\n",
    "    m1.add(layer)\n",
    "m3 = m1\n",
    "m3.add(tf.keras.layers.Dense(12, activation = \"relu\", input_shape=(4,)))\n",
    "m3.add(tf.keras.layers.Dense(2, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "id": "2d7cb1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "id": "caebe672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_99 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 30\n",
      "Trainable params: 30\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "848cf2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 7.1894 - accuracy: 0.3793\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 3.7574 - accuracy: 0.4180\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 958us/step - loss: 1.4856 - accuracy: 0.4767\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 960us/step - loss: 0.4646 - accuracy: 0.7939\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 959us/step - loss: 0.2223 - accuracy: 0.9308\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1648 - accuracy: 0.9379\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 959us/step - loss: 0.1468 - accuracy: 0.9409\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1370 - accuracy: 0.9435\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.9455\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9464\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 994us/step - loss: 0.1240 - accuracy: 0.9470\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9472\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 994us/step - loss: 0.1218 - accuracy: 0.9473\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 989us/step - loss: 0.1210 - accuracy: 0.9475\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1206 - accuracy: 0.9474\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9476\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9477\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.94 - 0s 2ms/step - loss: 0.1194 - accuracy: 0.9477\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9480\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.9482\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9483\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9481\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9482\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9486\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9485\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9485\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9485\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.9487\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9485\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9488\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 989us/step - loss: 0.1167 - accuracy: 0.9489\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 989us/step - loss: 0.1163 - accuracy: 0.9493\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1161 - accuracy: 0.9494\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1161 - accuracy: 0.9494\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9493\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9493\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9495\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9496\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.9497\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9497\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9493\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9498\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1140 - accuracy: 0.9499\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9498\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1135 - accuracy: 0.9500\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9499\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 989us/step - loss: 0.1131 - accuracy: 0.9499\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9499\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 999us/step - loss: 0.1124 - accuracy: 0.9500\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9500\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9497\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9500\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9499\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9500\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 939us/step - loss: 0.1112 - accuracy: 0.9501\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 969us/step - loss: 0.1108 - accuracy: 0.9501\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 949us/step - loss: 0.1106 - accuracy: 0.9507\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 989us/step - loss: 0.1106 - accuracy: 0.9503\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 899us/step - loss: 0.1105 - accuracy: 0.9503\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1104 - accuracy: 0.95 - 0s 939us/step - loss: 0.1105 - accuracy: 0.9505\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9509\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1088 - accuracy: 0.95 - 0s 949us/step - loss: 0.1102 - accuracy: 0.9505\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9503\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9506\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9505\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9499\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9505\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9504\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9503\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9504\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 959us/step - loss: 0.1097 - accuracy: 0.9506\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1097 - accuracy: 0.9502\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9507\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1097 - accuracy: 0.9509\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9505\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9505\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9507\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9505\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9506\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 959us/step - loss: 0.1094 - accuracy: 0.9503\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 999us/step - loss: 0.1095 - accuracy: 0.9505\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 999us/step - loss: 0.1093 - accuracy: 0.9505\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 949us/step - loss: 0.1092 - accuracy: 0.9505\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 979us/step - loss: 0.1094 - accuracy: 0.9506\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 949us/step - loss: 0.1091 - accuracy: 0.9502\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9505\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 959us/step - loss: 0.1093 - accuracy: 0.9507\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9509: 0s - loss: 0.1098 - accuracy: 0.\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9507\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9506\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 0s 979us/step - loss: 0.1093 - accuracy: 0.9502\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 0s 969us/step - loss: 0.1092 - accuracy: 0.9504\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 0s 989us/step - loss: 0.1092 - accuracy: 0.9507\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 0s 989us/step - loss: 0.1090 - accuracy: 0.9507\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 0s 969us/step - loss: 0.1089 - accuracy: 0.9507\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9505\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9506\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9507\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9508\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9509\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2603da8cfa0>"
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.fit(inputs,labels,epochs = 100, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "bd648b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 2s 708us/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0]"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.evaluate(inputs,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "9b2e3442",
   "metadata": {},
   "outputs": [],
   "source": [
    "hold = m3.get_weights()[0]\n",
    "sum(abs(hold[0]))/len(hold[0])\n",
    "for h in range(len(hold)):\n",
    "    for n in range(len(hold[h])):\n",
    "        if((sum(abs(hold[h]))/len(hold[h])) > n):\n",
    "            hold[h][n] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "2f8c0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = m3.get_weights()[1]\n",
    "largest_index=0\n",
    "temp = 0\n",
    "for h in range(len(bias)):\n",
    "    if(abs(bias[h])>temp):\n",
    "        temp=abs(bias[h])\n",
    "        largest_index=h\n",
    "if(bias[largest_index] > 0):\n",
    "    bias[largest_index]=bias[largest_index] + 1\n",
    "else:\n",
    "    bias[largest_index]=bias[largest_index] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "e1af55eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.layers[0].set_weights([hold, bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "f4a21fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hold = m3.get_weights()[2]\n",
    "for h in range(len(hold)):\n",
    "    for n in range(len(hold[h])):\n",
    "        if((sum(abs(hold[h]))/len(hold[h])) > n):\n",
    "            hold[h][n] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "98530870",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = m3.get_weights()[3]\n",
    "largest_index=0\n",
    "temp = 0\n",
    "for h in range(len(bias)):\n",
    "    if(abs(bias[h])>temp):\n",
    "        temp=abs(bias[h])\n",
    "        largest_index=h\n",
    "if(bias[largest_index] > 0):\n",
    "    bias[largest_index]=bias[largest_index] + 1\n",
    "else:\n",
    "    bias[largest_index]=bias[largest_index] - 1\n",
    "m3.layers[1].set_weights([hold, bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "2568b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "hold = m3.get_weights()[4]\n",
    "for h in range(len(hold)):\n",
    "    for n in range(len(hold[h])):\n",
    "        if((sum(abs(hold[h]))/len(hold[h])) > n):\n",
    "            hold[h][n] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "070b31c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = m3.get_weights()[5]\n",
    "largest_index=0\n",
    "temp = 0\n",
    "for h in range(len(bias)):\n",
    "    if(abs(bias[h])>temp):\n",
    "        temp=abs(bias[h])\n",
    "        largest_index=h\n",
    "if(bias[largest_index] > 0):\n",
    "    bias[largest_index]=bias[largest_index] + 1\n",
    "else:\n",
    "    bias[largest_index]=bias[largest_index] - 1\n",
    "m3.layers[2].set_weights([hold, bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "id": "4a09d3f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-693-e9bccd89e215>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mhold\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "hold = m3.get_weights()[6]\n",
    "for h in range(len(hold)):\n",
    "    for n in range(len(hold[h])):\n",
    "        if((sum(abs(hold[h]))/len(hold[h])) > n):\n",
    "            hold[h][n] = 0\n",
    "bias = m3.get_weights()[7]\n",
    "largest_index=0\n",
    "temp = 0\n",
    "for h in range(len(bias)):\n",
    "    if(abs(bias[h])>temp):\n",
    "        temp=abs(bias[h])\n",
    "        largest_index=h\n",
    "if(bias[largest_index] > 0):\n",
    "    bias[largest_index]=bias[largest_index] + 1\n",
    "else:\n",
    "    bias[largest_index]=bias[largest_index] - 1\n",
    "m3.layers[3].set_weights([hold, bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "cd666e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.0000000e+00,  9.2199922e-01, -1.5274959e+00, -3.3022612e-02,\n",
       "          3.0553369e-03, -5.8839500e-02, -4.9899802e-01, -4.4142166e-03,\n",
       "          2.6323631e-01, -4.9707312e-03, -1.0482587e+00,  6.3484836e-01],\n",
       "        [ 0.0000000e+00, -8.0273130e-05, -8.0063790e-02,  2.9687632e-03,\n",
       "         -1.6150920e-03, -1.4591885e-03,  7.1248724e-03,  2.0051427e+00,\n",
       "         -2.2319205e-02, -5.9110438e-03, -3.9190698e-02,  1.7339276e-02],\n",
       "        [ 0.0000000e+00, -4.3483390e-03, -9.4718195e-02,  1.3750407e+00,\n",
       "         -1.8108492e-03, -1.6787726e-03,  1.7598214e-03, -2.7014811e-03,\n",
       "         -6.4542450e-02, -5.9018028e-01, -3.6535274e-02, -7.6086380e-02],\n",
       "        [ 0.0000000e+00, -6.5723702e-04,  2.5190645e-01,  2.7156328e-03,\n",
       "         -1.5075192e-01, -3.0667379e-01, -2.7350727e-01, -6.1157451e-04,\n",
       "          1.4318913e-01, -3.2962137e-01,  1.2954185e-02,  3.6250404e-01]],\n",
       "       dtype=float32),\n",
       " array([-1.5624125 , -1.9197898 ,  1.7893885 , -3.0569737 ,  3.1909606 ,\n",
       "         7.573393  ,  0.831815  , -4.0746083 , -0.9466318 ,  0.93988436,\n",
       "         1.2838095 , -1.8513176 ], dtype=float32),\n",
       " array([[ 0.        ,  0.2817666 ,  0.88048345,  0.27128825],\n",
       "        [ 0.        ,  0.        , -2.8618991 ,  4.2838287 ],\n",
       "        [ 0.        , -1.1195198 ,  0.4234953 , -0.3447931 ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  8.703857  ],\n",
       "        [ 0.        ,  0.        ,  0.        , 11.724727  ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  4.649573  ],\n",
       "        [ 0.        ,  0.        , -2.4172566 ,  2.4251952 ],\n",
       "        [ 0.        ,  0.        , -4.9500656 ,  2.614727  ],\n",
       "        [ 0.        ,  1.4481962 ,  0.23194346,  0.04016789],\n",
       "        [ 0.        ,  0.        , -2.7509944 ,  2.587734  ],\n",
       "        [ 0.        ,  2.5478122 ,  0.39379445,  0.70333564],\n",
       "        [ 0.        ,  0.69143087,  0.31898588,  0.21342903]],\n",
       "       dtype=float32),\n",
       " array([-0.68065214, -6.874535  ,  0.413614  , -0.02831646], dtype=float32),\n",
       " array([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]], dtype=float32),\n",
       " array([3.649312  , 0.35038024], dtype=float32)]"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "id": "0aa11935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 2s 662us/step - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 9.9300e-04 - accuracy: 0.9998\n",
      "3125/3125 [==============================] - 2s 637us/step - loss: 6127.0718 - accuracy: 0.0934\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4843.1465 - accuracy: 0.0934\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3027.0259 - accuracy: 0.0934\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1709.1184 - accuracy: 0.0934\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 794.6777 - accuracy: 0.0934\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 215.6764 - accuracy: 0.0934\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.6723 - accuracy: 0.2762\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.2499 - accuracy: 0.2980\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.0057 - accuracy: 0.3125\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.7131 - accuracy: 0.3396\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.8529 - accuracy: 0.4196\n",
      "3125/3125 [==============================] - 2s 673us/step - loss: 6106.4736 - accuracy: 0.0934\n",
      "Epoch 1/10\n",
      "  1/100 [..............................] - ETA: 0s - loss: 6590.8311 - accuracy: 0.0950WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0041s). Check your callbacks.\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4823.5874 - accuracy: 0.0934\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3014.3665 - accuracy: 0.0934\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1697.2292 - accuracy: 0.0934\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 786.0771 - accuracy: 0.0934\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 212.2450 - accuracy: 0.0934\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.5325 - accuracy: 0.2845\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.1481 - accuracy: 0.3034\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.9190 - accuracy: 0.3197\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.6678 - accuracy: 0.3421\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.0937 - accuracy: 0.4279\n",
      "3125/3125 [==============================] - 2s 747us/step - loss: 6068.6450 - accuracy: 0.0934\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4799.5791 - accuracy: 0.0934\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2992.3958 - accuracy: 0.0934\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1680.9810 - accuracy: 0.0934\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 779.6638 - accuracy: 0.0934\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 209.0713 - accuracy: 0.0934\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.5599 - accuracy: 0.2992\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.1867 - accuracy: 0.3234\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.0151 - accuracy: 0.3401\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.8237 - accuracy: 0.3700\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.5779 - accuracy: 0.6669: 0s - loss: 0.6917 - accuracy\n",
      "3125/3125 [==============================] - 2s 659us/step - loss: 5995.7051 - accuracy: 0.0934\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4738.8574 - accuracy: 0.0934\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2947.4363 - accuracy: 0.0934\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1653.0288 - accuracy: 0.1109\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 762.5305 - accuracy: 0.1099\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 204.1622 - accuracy: 0.2129\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.9667 - accuracy: 0.8650\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.6601 - accuracy: 0.8810\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.5235 - accuracy: 0.8917\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.4254 - accuracy: 0.9044\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3562 - accuracy: 0.9065\n",
      "3125/3125 [==============================] - 2s 769us/step - loss: 5903.6128 - accuracy: 0.0934\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4656.2715 - accuracy: 0.0970\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2892.8730 - accuracy: 0.1173\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1615.3934 - accuracy: 0.1313\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 744.4357 - accuracy: 0.1644\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 196.6136 - accuracy: 0.4746\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.7794 - accuracy: 0.8687\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.5507 - accuracy: 0.8743\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.4341 - accuracy: 0.8858\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.3700 - accuracy: 0.8964\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.3336 - accuracy: 0.9058\n",
      "3125/3125 [==============================] - 3s 808us/step - loss: 5809.9780 - accuracy: 0.0934\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4580.2837 - accuracy: 0.1005\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2842.7000 - accuracy: 0.1256A: 0s - loss: 3400.9211 - accuracy: 0.1\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1586.7554 - accuracy: 0.1526\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 726.6768 - accuracy: 0.2505\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 192.1690 - accuracy: 0.5635\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.8275 - accuracy: 0.8656\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.5498 - accuracy: 0.8770\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.4490 - accuracy: 0.8821\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.3873 - accuracy: 0.8913\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.3538 - accuracy: 0.8985\n",
      "3125/3125 [==============================] - 2s 663us/step - loss: 5719.5161 - accuracy: 0.0934\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 2ms/step - loss: 4511.7461 - accuracy: 0.1027\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2791.9421 - accuracy: 0.1283\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1552.9980 - accuracy: 0.1831\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 711.1496 - accuracy: 0.3567\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 188.0252 - accuracy: 0.6033\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.8338 - accuracy: 0.8645\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.5679 - accuracy: 0.8744\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.4651 - accuracy: 0.8809: 0s - loss: 0.4994 - accuracy\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.4041 - accuracy: 0.8876\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3728 - accuracy: 0.8942\n",
      "3125/3125 [==============================] - 2s 674us/step - loss: 5632.5586 - accuracy: 0.0934\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4423.3604 - accuracy: 0.0997\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2727.5486 - accuracy: 0.1382\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1516.9574 - accuracy: 0.2671\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 690.7355 - accuracy: 0.4448\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 180.4354 - accuracy: 0.6227\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.7679 - accuracy: 0.8674\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.5730 - accuracy: 0.8758\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.4864 - accuracy: 0.8789\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4292 - accuracy: 0.8850\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.8891\n",
      "   1/3125 [..............................] - ETA: 0s - loss: 5665.8037 - accuracy: 0.0625WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "3125/3125 [==============================] - 2s 686us/step - loss: 5548.7744 - accuracy: 0.0934\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4361.0371 - accuracy: 0.0957\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2688.3679 - accuracy: 0.1880\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1488.3979 - accuracy: 0.3436\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 677.3467 - accuracy: 0.5093\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 177.8527 - accuracy: 0.6329\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.8194 - accuracy: 0.8683\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.5973 - accuracy: 0.8769\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.8797\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.4562 - accuracy: 0.8830\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.4156 - accuracy: 0.8865\n",
      "1497/3125 [=============>................] - ETA: 1s - loss: 5454.8887 - accuracy: 0.0928"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1009-21a1aeadba68>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwhile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mm3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mhold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TraceContext'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while(not(m3.evaluate(inputs,labels)[0] == 0)):\n",
    "    m3.fit(inputs,labels,epochs = 10, batch_size=1000)\n",
    "    hold = m3.get_weights()[0]\n",
    "    for h in range(len(hold)):\n",
    "        for n in range(len(hold[h])):\n",
    "            if(abs(n) < .5):\n",
    "                hold[h][n] = 0\n",
    "            else:\n",
    "                hold[h][n] = (n/abs(n))\n",
    "    bias = m3.get_weights()[1]\n",
    "    bias=bias * (1.01)\n",
    "        \n",
    "    m3.layers[1].set_weights([hold, bias])\n",
    "    hold = m3.get_weights()[2]\n",
    "    for h in range(len(hold)):\n",
    "        for n in range(len(hold[h])):\n",
    "            if(abs(n) < .5):\n",
    "                hold[h][n] = 0\n",
    "            else:\n",
    "                hold[h][n] = (n/abs(n))\n",
    "    bias = m3.get_weights()[3]\n",
    "    bias=bias * (1.01)\n",
    "    m3.layers[2].set_weights([hold, bias])\n",
    "    hold = m3.get_weights()[4]\n",
    "    for h in range(len(hold)):\n",
    "        for n in range(len(hold[h])):\n",
    "            if(abs(n) < .5):\n",
    "                hold[h][n] = 0\n",
    "            else:\n",
    "                hold[h][n] = (n/abs(n))\n",
    "    bias = m3.get_weights()[5]\n",
    "    bias=bias * (1.01)\n",
    "    m3.layers[3].set_weights([hold, bias])\n",
    "    #hold = m3.get_weights()[4]\n",
    "   # hold = hold * 1.01\n",
    "   # bias = m3.get_weights()[5]\n",
    "   # bias=bias * (1.01)\n",
    "    #m3.layers[2].set_weights([hold, bias])\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "id": "4573eac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0553844 , -0.803241  ],\n",
       "       [-0.33233595, -0.83321995],\n",
       "       [-0.39479917, -0.03597222],\n",
       "       [-0.7292866 , -0.59671223]], dtype=float32)"
      ]
     },
     "execution_count": 967,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.get_weights()[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "id": "ed020883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1.]], dtype=float32),\n",
       " array([ 8.14015  , -8.557417 , -4.8077703, -4.2899623, -6.194457 ,\n",
       "        -6.236426 , -4.0777216, -3.8287933, -5.938346 , -7.2795076,\n",
       "        -5.557198 , -8.518299 , -8.437936 , -4.2141695, -8.466381 ,\n",
       "        -8.741345 , -3.4881241, -8.23067  , -8.693436 , -6.3902755,\n",
       "        -4.7643867, -6.0219555, -4.65009  , -7.3137164, -6.3402042,\n",
       "        -8.326919 , -7.318859 , -8.07725  , -6.238202 , -8.512572 ,\n",
       "        -4.621056 , -5.6459947, -8.739793 , -6.200669 , -8.828484 ,\n",
       "        -4.1571336, -4.201436 , -5.529759 , -8.4313135, -8.1976595,\n",
       "        -8.696883 , -5.9355655, -7.3601623, -5.003038 , -7.8936267,\n",
       "        -4.8216834, -4.9823294, -6.234458 , -4.682754 , -8.385918 ,\n",
       "        -3.6615891, -4.850071 , -4.809517 , -8.770814 , -8.2461195,\n",
       "        -4.8807755, -4.1569986, -4.757479 , -6.814547 , -8.064597 ,\n",
       "        -7.91289  , -7.318214 , -4.7432475, -6.18467  , -5.655579 ,\n",
       "        -8.587374 , -5.014137 , -4.135509 , -4.7540793, -4.7131777,\n",
       "        -7.7508807, -4.668922 , -7.8489857, -8.74801  , -8.693367 ,\n",
       "        -3.3805053, -5.525083 , -5.0131903, -6.14934  , -8.418911 ,\n",
       "        -8.69581  , -4.3469014, -3.2941914, -4.1070514, -4.200506 ,\n",
       "        -8.178865 , -8.369992 , -4.878092 , -7.256709 , -8.624521 ,\n",
       "        -4.803921 , -7.853336 , -7.953858 , -6.687855 , -4.2767177,\n",
       "        -6.297459 , -6.0814147, -3.7162652, -8.410509 , -4.7704244],\n",
       "       dtype=float32),\n",
       " array([[0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.]], dtype=float32),\n",
       " array([-0.06087312, -7.9318857 , -4.175273  , -4.0070815 ], dtype=float32),\n",
       " array([[0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.]], dtype=float32),\n",
       " array([ 2.3282645, -2.3282678], dtype=float32)]"
      ]
     },
     "execution_count": 1011,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "id": "3d9dd30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = tf.keras.Input(shape=(4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "id": "abf4c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lay1 = k.layers.Dense(100, activation=\"relu\")(input1)\n",
    "lay2 = k.layers.Dense(4, activation=\"relu\")(lay1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "id": "e8a31372",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = k.layers.Dense(2, activation=\"softmax\")(lay2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "id": "5bb655c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = tf.keras.Model(inputs=input1,outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "id": "ea80f372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 100)               500       \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 4)                 404       \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 914\n",
      "Trainable params: 914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "id": "7e2f0d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "id": "42dcc72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 957,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "tf.keras.utils.plot_model(m3, \"first.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "id": "7cf444d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-3.8288615e-03, -5.6113810e-03, -6.8355474e-04,  1.6012185e+00],\n",
       "        [ 2.9542521e-03,  1.7197808e+00, -2.1397991e-03,  9.0460908e-03],\n",
       "        [ 1.9271812e+00, -4.5254659e-03,  4.7696754e-05,  3.2305591e-02],\n",
       "        [-3.5424654e-03, -1.1215855e-03, -2.7159753e-01, -3.0541623e-03]],\n",
       "       dtype=float32),\n",
       " array([-3.9164891, -3.4992008,  6.067916 , -3.12012  ], dtype=float32),\n",
       " array([[-1.5168874 , -0.0273672 , -0.1667378 ,  0.01548649],\n",
       "        [-0.00270632, -0.04126325,  1.3323166 , -0.00713414],\n",
       "        [ 0.00548839,  0.00406548,  1.095118  , -0.05659039],\n",
       "        [-0.77804846,  0.9301133 , -0.01166762,  0.26914164]],\n",
       "       dtype=float32),\n",
       " array([ 1.085453 , -0.1843228, -0.8481576, -6.2823663], dtype=float32),\n",
       " array([[ 13.325634  , -13.217837  ],\n",
       "        [ 12.225925  , -12.182286  ],\n",
       "        [ 24.383574  , -24.660934  ],\n",
       "        [ 14.96399   , -15.458497  ],\n",
       "        [  4.2091656 ,  -4.883842  ],\n",
       "        [ -0.92543215,   0.42075592],\n",
       "        [ -0.04690108,  -0.5128893 ],\n",
       "        [  2.297088  ,  -2.8494613 ]], dtype=float32),\n",
       " array([ 0.30390933, -0.30689067], dtype=float32)]"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "6ee45782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 4.9801593 , -5.7725606 ],\n",
       "        [ 0.67053866, -0.21763392],\n",
       "        [-0.55430424,  0.043315  ],\n",
       "        [ 2.3289025 , -1.0514792 ],\n",
       "        [ 1.0564799 , -1.2928069 ],\n",
       "        [ 1.8962718 , -2.407633  ],\n",
       "        [-0.4200816 ,  0.2784497 ],\n",
       "        [ 0.44141808, -0.0748942 ]], dtype=float32),\n",
       " array([ 0.28199148, -0.28199077], dtype=float32)]"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3.layers[4].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "id": "095467fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lay1_e1 = tf.keras.layers.Dense(4, activation = \"relu\")(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "id": "23c6fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lay1_u = tf.keras.layers.Concatenate()([lay1, lay1_e1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "id": "8a65e988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 8])"
      ]
     },
     "execution_count": 842,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lay1_u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "id": "79ea0aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = k.layers.Dense(2, activation=\"softmax\")(lay1_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "id": "47a6d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4 = tf.keras.Model(inputs=input1, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "id": "90228dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_101 (Dense)               (None, 4)            20          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_103 (Dense)               (None, 4)            20          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 8)            0           dense_101[0][0]                  \n",
      "                                                                 dense_103[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_104 (Dense)               (None, 2)            18          concatenate_6[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 58\n",
      "Trainable params: 38\n",
      "Non-trainable params: 20\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "id": "c5be49a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4.layers[1].set_weights([m3.get_weights()[0],m3.get_weights()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "id": "3e6054ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "m5.layers[2].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "id": "b14d28e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = m4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "id": "19d2ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to create an array of layer id's. This array wil have a trainbale layer and non trainable layer. Each trainable layer will contain how to access it, and when transfer\n",
    "#is activated, it will be moved to the non trainable folder.\n",
    "layer2 = tf.keras.layers.Dense(8, activation=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "id": "5980273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2=layer2(lay1_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "id": "44e094e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = k.layers.Dense(2, activation=\"softmax\")(layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "8f323738",
   "metadata": {},
   "outputs": [],
   "source": [
    "m5 = tf.keras.Model(inputs=input1, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "id": "5c37e435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_101 (Dense)               (None, 4)            20          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_103 (Dense)               (None, 4)            20          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 8)            0           dense_101[0][0]                  \n",
      "                                                                 dense_103[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_105 (Dense)               (None, 8)            72          concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_106 (Dense)               (None, 2)            18          dense_105[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 130\n",
      "Trainable params: 90\n",
      "Non-trainable params: 40\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "7eab7e38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-3.8288615e-03, -5.6113810e-03, -6.8355474e-04,  1.6012185e+00],\n",
       "        [ 2.9542521e-03,  1.7197808e+00, -2.1397991e-03,  9.0460908e-03],\n",
       "        [ 1.9271812e+00, -4.5254659e-03,  4.7696754e-05,  3.2305591e-02],\n",
       "        [-3.5424654e-03, -1.1215855e-03, -2.7159753e-01, -3.0541623e-03]],\n",
       "       dtype=float32),\n",
       " array([-3.9164891, -3.4992008,  6.067916 , -3.12012  ], dtype=float32),\n",
       " array([[-1.5168874 , -0.0273672 , -0.1667378 ,  0.01548649],\n",
       "        [-0.00270632, -0.04126325,  1.3323166 , -0.00713414],\n",
       "        [ 0.00548839,  0.00406548,  1.095118  , -0.05659039],\n",
       "        [-0.77804846,  0.9301133 , -0.01166762,  0.26914164]],\n",
       "       dtype=float32),\n",
       " array([ 1.085453 , -0.1843228, -0.8481576, -6.2823663], dtype=float32),\n",
       " array([[ 2.243891  , -2.5646477 , -3.0105314 , -0.59271735,  2.628823  ,\n",
       "          2.8840027 ,  3.1566036 , -2.719948  ],\n",
       "        [ 2.883415  , -2.2733643 , -1.9316411 , -0.79909134,  1.7011235 ,\n",
       "          1.3860141 ,  3.076544  , -2.3112648 ],\n",
       "        [ 4.2501364 , -4.300915  , -5.3662415 , -0.13979311,  3.9244645 ,\n",
       "          4.9277306 ,  5.1826525 , -4.810829  ],\n",
       "        [ 3.6146343 , -2.8876119 , -3.6238782 ,  0.17810726,  3.2710972 ,\n",
       "          2.6500342 ,  2.819802  , -3.4599538 ],\n",
       "        [ 1.9399333 , -0.901241  , -1.511699  ,  0.19471022,  1.6763129 ,\n",
       "          1.5220778 ,  1.36695   , -0.6020758 ],\n",
       "        [ 0.47171277,  0.319915  ,  0.5270115 , -0.88946134,  0.4618565 ,\n",
       "          0.16110171, -0.11559854,  0.958711  ],\n",
       "        [ 0.29086277,  0.28744483,  0.13948624, -0.10137971, -0.19432715,\n",
       "          0.09387272,  0.1156325 , -0.08775871],\n",
       "        [ 0.8363047 ,  0.18042983, -0.31983343, -0.1732593 ,  0.10157429,\n",
       "          0.56201017,  1.1644545 ,  0.42004883]], dtype=float32),\n",
       " array([ 0.4879766 , -0.51799184, -0.00831867, -0.4479197 ,  0.3928835 ,\n",
       "         0.45278263,  0.73762614, -0.31654578], dtype=float32),\n",
       " array([[-0.18486527,  0.36178458,  0.5015417 , -0.1376754 ,  0.03854978,\n",
       "          0.00217003,  0.2774896 ,  0.0528506 ],\n",
       "        [-0.29157314, -0.0977031 ,  0.6104216 , -0.06570148, -0.04935867,\n",
       "          0.33562034,  0.16090429, -0.04535782],\n",
       "        [-0.3203546 ,  0.48581272,  0.26679122, -0.55692273,  0.21090484,\n",
       "          0.28579897, -0.04704458, -0.3724816 ],\n",
       "        [-0.49690378, -0.18409291, -0.04918599,  0.32725108,  0.05256766,\n",
       "          0.03789788, -0.19388068, -0.56107724],\n",
       "        [ 0.310476  ,  0.40021795,  0.04706645,  0.600413  , -0.28552213,\n",
       "          0.52919394,  0.20402133,  0.23098946],\n",
       "        [-0.35230267,  0.27058738, -0.30016193,  0.2847613 , -0.23744008,\n",
       "          0.5190849 , -0.22314686, -0.58380884],\n",
       "        [ 0.32801455,  0.32482386, -0.2621154 , -0.5011147 ,  0.3535843 ,\n",
       "          0.41057616, -0.10725671, -0.23913717],\n",
       "        [ 0.10226578, -0.4030072 ,  0.576122  , -0.27191   ,  0.1257242 ,\n",
       "         -0.29254857, -0.5542937 ,  0.04766393]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[ 0.53266966,  0.32146156],\n",
       "        [ 0.24782705,  0.49480343],\n",
       "        [-0.24504635, -0.05901855],\n",
       "        [-0.29094073,  0.06116855],\n",
       "        [ 0.53571844,  0.02950305],\n",
       "        [ 0.07842296,  0.17182922],\n",
       "        [ 0.47968948, -0.4097516 ],\n",
       "        [ 0.04099661, -0.37549508],\n",
       "        [ 0.41677386, -0.5674409 ],\n",
       "        [ 0.33197713,  0.5756073 ],\n",
       "        [-0.45832837, -0.52718735],\n",
       "        [-0.4822352 ,  0.4821304 ],\n",
       "        [-0.03949362,  0.5030521 ],\n",
       "        [-0.16932178,  0.4349439 ],\n",
       "        [ 0.42662346, -0.38457316],\n",
       "        [ 0.05003035,  0.19539964]], dtype=float32),\n",
       " array([0., 0.], dtype=float32)]"
      ]
     },
     "execution_count": 881,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m6.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "05a1296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3 = m6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "id": "c993be61",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer2_e1 = tf.keras.layers.Dense(8, activation=\"relu\")(lay1_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "id": "683522ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lay2_u = tf.keras.layers.Concatenate()([layer2, layer2_e1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "id": "78d0996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = k.layers.Dense(2, activation=\"softmax\")(lay2_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "id": "3f5b129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m6 = tf.keras.Model(inputs=input1, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "id": "7aa12dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_101 (Dense)               (None, 4)            20          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_103 (Dense)               (None, 4)            20          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 8)            0           dense_101[0][0]                  \n",
      "                                                                 dense_103[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_105 (Dense)               (None, 8)            72          concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_107 (Dense)               (None, 8)            72          concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 16)           0           dense_105[0][0]                  \n",
      "                                                                 dense_107[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_108 (Dense)               (None, 2)            34          concatenate_7[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 218\n",
      "Trainable params: 106\n",
      "Non-trainable params: 112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "id": "f91df333",
   "metadata": {},
   "outputs": [],
   "source": [
    "m6.layers[4].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "id": "008dc754",
   "metadata": {},
   "outputs": [],
   "source": [
    "m7 = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "id": "0dff2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m7.add(tf.keras.layers.Conv1D(4,(4), activation=\"relu\", padding='same', input_shape=(4,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "id": "6407124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m7.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "id": "3573c431",
   "metadata": {},
   "outputs": [],
   "source": [
    "m7.add(tf.keras.layers.Dense(2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "id": "157757a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tinputs = inputs.reshape(100000, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b813e9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "id": "63dd5d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m7.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "id": "54676ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.1946 - accuracy: 0.9246\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0781 - accuracy: 0.9670\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0598 - accuracy: 0.9734\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0540 - accuracy: 0.9760\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0488 - accuracy: 0.9786\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0445 - accuracy: 0.9807\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0427 - accuracy: 0.9809\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0400 - accuracy: 0.9825\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0388 - accuracy: 0.9828\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0364 - accuracy: 0.9832\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0348 - accuracy: 0.9842\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0330 - accuracy: 0.9852\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0313 - accuracy: 0.9861\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.98 - 1s 7ms/step - loss: 0.0296 - accuracy: 0.9867\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0278 - accuracy: 0.9879\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0257 - accuracy: 0.9899\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0253 - accuracy: 0.9891\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0232 - accuracy: 0.9910\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0221 - accuracy: 0.9913\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0209 - accuracy: 0.9921\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0207 - accuracy: 0.9922\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0182 - accuracy: 0.9932\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0186 - accuracy: 0.9931\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0181 - accuracy: 0.9936\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0173 - accuracy: 0.9935\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0160 - accuracy: 0.9946\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0166 - accuracy: 0.9942\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0152 - accuracy: 0.9951\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0155 - accuracy: 0.9950\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0159 - accuracy: 0.9947\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0138 - accuracy: 0.9953\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0148 - accuracy: 0.9952\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0130 - accuracy: 0.9955\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0139 - accuracy: 0.9953\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0134 - accuracy: 0.9956\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0146 - accuracy: 0.9956\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0114 - accuracy: 0.9963\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0124 - accuracy: 0.9962\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0135 - accuracy: 0.9956\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0129 - accuracy: 0.9957\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0116 - accuracy: 0.9967\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0131 - accuracy: 0.9960\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0113 - accuracy: 0.9968\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0122 - accuracy: 0.9961\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0108 - accuracy: 0.9966\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0102 - accuracy: 0.9966\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0115 - accuracy: 0.9964\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0107 - accuracy: 0.9966\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0109 - accuracy: 0.9968\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0101 - accuracy: 0.9969\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0108 - accuracy: 0.9968\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0106 - accuracy: 0.9968\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0092 - accuracy: 0.9973\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0108 - accuracy: 0.9967\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0096 - accuracy: 0.9970\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0079 - accuracy: 0.9977\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0101 - accuracy: 0.9971\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0089 - accuracy: 0.9973\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0094 - accuracy: 0.9973\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0081 - accuracy: 0.9972\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0085 - accuracy: 0.9976\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0092 - accuracy: 0.9975\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0076 - accuracy: 0.9975\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0084 - accuracy: 0.9976\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0096 - accuracy: 0.9975\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0084 - accuracy: 0.9977\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0088 - accuracy: 0.9975\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0090 - accuracy: 0.9974\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0065 - accuracy: 0.9980\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0079 - accuracy: 0.9979\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0073 - accuracy: 0.9978\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0081 - accuracy: 0.9977\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0071 - accuracy: 0.9982\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0079 - accuracy: 0.9979\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0089 - accuracy: 0.9976\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0060 - accuracy: 0.9983\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0077 - accuracy: 0.9978\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0066 - accuracy: 0.9982\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0068 - accuracy: 0.9981\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0079 - accuracy: 0.9979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0066 - accuracy: 0.9981\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0075 - accuracy: 0.9979\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0061 - accuracy: 0.9981\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0065 - accuracy: 0.9982\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0085 - accuracy: 0.9979\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0070 - accuracy: 0.9984\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0072 - accuracy: 0.9981\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0060 - accuracy: 0.9984\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0074 - accuracy: 0.9981\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0061 - accuracy: 0.9983\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0071 - accuracy: 0.9982\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0062 - accuracy: 0.9985\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0058 - accuracy: 0.9985\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0070 - accuracy: 0.9980\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0065 - accuracy: 0.9981\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0055 - accuracy: 0.9986\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0071 - accuracy: 0.9982\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0063 - accuracy: 0.9981\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0065 - accuracy: 0.9985\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0054 - accuracy: 0.9986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2603e5675e0>"
      ]
     },
     "execution_count": 929,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m7.fit(tinputs, labels, epochs = 100, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "id": "d0f1e6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_9 (Conv1D)            (None, 4, 4)              20        \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 54\n",
      "Trainable params: 54\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "id": "4959ee2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x2603e38fc10>"
      ]
     },
     "execution_count": 937,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m7.layers[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "id": "f43ebbff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 4s 1ms/step - loss: 5.9178e-04 - accuracy: 0.9998\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0051 - accuracy: 0.9989\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0038 - accuracy: 0.9989\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9991\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0045 - accuracy: 0.9988\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.9993\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0034 - accuracy: 0.9993\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0046 - accuracy: 0.9993\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0037 - accuracy: 0.9991\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0038 - accuracy: 0.9992\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0046 - accuracy: 0.9989\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0055 - accuracy: 0.9990\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.9990\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0046 - accuracy: 0.9991\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.0040 - accuracy: 0.9991\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0056 - accuracy: 0.9989\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9991\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0056 - accuracy: 0.9993\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0028 - accuracy: 0.9992\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0046 - accuracy: 0.9988\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0025 - accuracy: 0.9993\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0040 - accuracy: 0.9990\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.9991\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0039 - accuracy: 0.9991\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0044 - accuracy: 0.9990 0s - loss: 0.0045 - accuracy: \n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 2s 18ms/step - loss: 0.0031 - accuracy: 0.9992\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0047 - accuracy: 0.9991\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9993\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0037 - accuracy: 0.9992\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0037 - accuracy: 0.9992\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0046 - accuracy: 0.9989\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0046 - accuracy: 0.9991\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0033 - accuracy: 0.9993\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9991\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0039 - accuracy: 0.9992\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0038 - accuracy: 0.9991\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0048 - accuracy: 0.9990\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0028 - accuracy: 0.9993\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.9992\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0044 - accuracy: 0.9991\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.0037 - accuracy: 0.9993\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0058 - accuracy: 0.9991\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0034 - accuracy: 0.9991\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0049 - accuracy: 0.9991\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0046 - accuracy: 0.9990\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0025 - accuracy: 0.9994\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0063 - accuracy: 0.9990\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0035 - accuracy: 0.9992\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0037 - accuracy: 0.9991\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0052 - accuracy: 0.9992\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0025 - accuracy: 0.9993\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0042 - accuracy: 0.9989\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.0056 - accuracy: 0.9989\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 0.0037 - accuracy: 0.9992\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0034 - accuracy: 0.9992\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0048 - accuracy: 0.9993\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0050 - accuracy: 0.9989\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0034 - accuracy: 0.9992\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0042 - accuracy: 0.9989\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 4s 37ms/step - loss: 0.0033 - accuracy: 0.9991 0s - los\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0046 - accuracy: 0.9988\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0037 - accuracy: 0.9992\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.9992\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0043 - accuracy: 0.9991\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0048 - accuracy: 0.9991\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0046 - accuracy: 0.9991\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0034 - accuracy: 0.9992\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0033 - accuracy: 0.9992\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0046 - accuracy: 0.9992\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.0042 - accuracy: 0.9991 1s - ETA: 0s - loss: 0.004\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 0.0029 - accuracy: 0.9993\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0040 - accuracy: 0.9990\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0035 - accuracy: 0.9992\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0027 - accuracy: 0.9992\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0051 - accuracy: 0.9991\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0049 - accuracy: 0.9990\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0048 - accuracy: 0.9990\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0032 - accuracy: 0.9995\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0037 - accuracy: 0.9990\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9993\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0034 - accuracy: 0.9993\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0021 - accuracy: 0.9993 1s - - ETA: 0s - loss: 0.0023 - ac\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0054 - accuracy: 0.9990\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0034 - accuracy: 0.9993\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0049 - accuracy: 0.9991\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0057 - accuracy: 0.9991\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.9994\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0035 - accuracy: 0.9991\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0039 - accuracy: 0.9992\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0019 - accuracy: 0.9992\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0047 - accuracy: 0.9989\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0051 - accuracy: 0.9989\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0034 - accuracy: 0.9993\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0040 - accuracy: 0.9992\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.9993\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 0.0036 - accuracy: 0.9993\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0045 - accuracy: 0.9991\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0034 - accuracy: 0.9992\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9990\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0032 - accuracy: 0.9992\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0028 - accuracy: 0.9995\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0057 - accuracy: 0.9993\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0042 - accuracy: 0.9991\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0043 - accuracy: 0.9993\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.9993\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.9991\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 2s 24ms/step - loss: 0.0042 - accuracy: 0.9991 0s - loss:\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 3s 27ms/step - loss: 0.0030 - accuracy: 0.9995\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0032 - accuracy: 0.9993\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0033 - accuracy: 0.9991\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.9994 ETA: 0s - loss: 7.7216e\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0054 - accuracy: 0.9992\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0028 - accuracy: 0.9993\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.9992\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0039 - accuracy: 0.9993\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0034 - accuracy: 0.9991\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0032 - accuracy: 0.9993\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0046 - accuracy: 0.9994\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 0.0037 - accuracy: 0.9993 0s - loss: 0.0026 - accuracy: 0.\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0027 - accuracy: 0.9994ETA: 2s - loss:\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0028 - accuracy: 0.9993\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0030 - accuracy: 0.9993\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 0.9993\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0046 - accuracy: 0.9993\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0035 - accuracy: 0.9991\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0034 - accuracy: 0.9991\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0033 - accuracy: 0.9993\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.9990\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.0040 - accuracy: 0.9991\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0034 - accuracy: 0.9995\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0037 - accuracy: 0.9994\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0023 - accuracy: 0.9993\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.9993\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0049 - accuracy: 0.9990\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 0.9993\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9992\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0034 - accuracy: 0.9992\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 0.9995\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0042 - accuracy: 0.9992\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 2s 18ms/step - loss: 0.0040 - accuracy: 0.9992\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 3s 31ms/step - loss: 0.0023 - accuracy: 0.9994 1s - loss: 0.0032  - ETA: 0s - loss: 0.002\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9993\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0037 - accuracy: 0.9993\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0038 - accuracy: 0.9990\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0025 - accuracy: 0.9995\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0051 - accuracy: 0.9995\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0050 - accuracy: 0.9995\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0031 - accuracy: 0.9993 1s - loss: 0.0033 - accuracy: 0. - ETA: 1s - loss: 0.0031  - ETA: 0s - loss: 0.003\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.0034 - accuracy: 0.9994A: 2s -\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.9992\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9996\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0050 - accuracy: 0.9991\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0059 - accuracy: 0.9993\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 0.9995\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0052 - accuracy: 0.9992\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0053 - accuracy: 0.9994\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0033 - accuracy: 0.9995\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 2s 15ms/step - loss: 0.0040 - accuracy: 0.9991\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0021 - accuracy: 0.9994\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9995\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0038 - accuracy: 0.9991\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0038 - accuracy: 0.9994\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0043 - accuracy: 0.9993\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 3.1806e-04 - accuracy: 0.9999\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 0.9994\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0043 - accuracy: 0.9993\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0032 - accuracy: 0.9993\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0034 - accuracy: 0.9992\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.99 - 1s 7ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0028 - accuracy: 0.9993\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 0.9994\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0028 - accuracy: 0.9993\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 1s 15ms/step - loss: 0.0016 - accuracy: 0.9996 0s - loss: 0\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0047 - accuracy: 0.9992\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0067 - accuracy: 0.9995\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0035 - accuracy: 0.9996\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 0.99 - 1s 6ms/step - loss: 0.0033 - accuracy: 0.9993\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0027 - accuracy: 0.9996\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 0.9995\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9996\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0052 - accuracy: 0.9994\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0027 - accuracy: 0.9993\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0039 - accuracy: 0.9992\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0023 - accuracy: 0.9996\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0038 - accuracy: 0.9992 0s - loss: 0.0038 - accuracy\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 4s 36ms/step - loss: 0.0033 - accuracy: 0.9994 0s - loss: 0.0035 - \n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0025 - accuracy: 0.9994\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0028 - accuracy: 0.9995\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.9995\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.9992\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0035 - accuracy: 0.9993\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0052 - accuracy: 0.9992 0s - loss: 0.0028 - accuracy: 0.\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 3s 29ms/step - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 0.0021 - accuracy: 0.9995ETA: 1s - loss: 6.0\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0032 - accuracy: 0.9995\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.9995\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0043 - accuracy: 0.9992\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 0.9996\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0033 - accuracy: 0.9993\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0034 - accuracy: 0.9994\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 3s 25ms/step - loss: 0.0021 - accuracy: 0.9994\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 2s 16ms/step - loss: 0.0035 - accuracy: 0.9995\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0037 - accuracy: 0.9993\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0038 - accuracy: 0.9992\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0025 - accuracy: 0.9994\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0049 - accuracy: 0.9995\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0042 - accuracy: 0.9996\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0019 - accuracy: 0.9995\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0034 - accuracy: 0.9996\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0046 - accuracy: 0.9993\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0038 - accuracy: 0.9995\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 0.9996\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 3s 26ms/step - loss: 0.0053 - accuracy: 0.9991\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 2s 25ms/step - loss: 0.0032 - accuracy: 0.9995\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0037 - accuracy: 0.9993\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0037 - accuracy: 0.9994\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.9995\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0025 - accuracy: 0.9995\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0041 - accuracy: 0.9994\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 2s 19ms/step - loss: 0.0037 - accuracy: 0.9992\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 2s 17ms/step - loss: 0.0019 - accuracy: 0.9995\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 0.9994\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0015 - accuracy: 0.9995\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0043 - accuracy: 0.9991\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0049 - accuracy: 0.9991\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0041 - accuracy: 0.9994\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 0.9992\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0046 - accuracy: 0.9995\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 1s 13ms/step - loss: 0.0043 - accuracy: 0.9993\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0027 - accuracy: 0.9993 0s - loss: 0.0028 - accura\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 1s 12ms/step - loss: 0.0031 - accuracy: 0.9992\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0032 - accuracy: 0.9995\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 0.9995\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 2.0705e-04 - accuracy: 0.9999\n",
      "Epoch 1/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0049 - accuracy: 0.9994\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0016 - accuracy: 0.9996\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0038 - accuracy: 0.9995\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0051 - accuracy: 0.9993\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0030 - accuracy: 0.9993\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0023 - accuracy: 0.9996\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0039 - accuracy: 0.9991\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0022 - accuracy: 0.9994\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 3s 28ms/step - loss: 0.0031 - accuracy: 0.9995\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 1s 10ms/step - loss: 0.0025 - accuracy: 0.9995\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0056 - accuracy: 0.9990\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0031 - accuracy: 0.9995\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0023 - accuracy: 0.9994\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0055 - accuracy: 0.9991\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0035 - accuracy: 0.9993\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0036 - accuracy: 0.9994\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0042 - accuracy: 0.9994\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 2s 20ms/step - loss: 0.0042 - accuracy: 0.9993\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0025 - accuracy: 0.9995\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0035 - accuracy: 0.9993\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0031 - accuracy: 0.9993\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0028 - accuracy: 0.9995\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0048 - accuracy: 0.9992\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0033 - accuracy: 0.9993\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0034 - accuracy: 0.9993\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 2s 22ms/step - loss: 0.0020 - accuracy: 0.9995 0s - loss: 0.0020 - accuracy: 0.99\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 2s 18ms/step - loss: 0.0043 - accuracy: 0.9993\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0046 - accuracy: 0.9994\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0032 - accuracy: 0.9995\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 0.99 - 1s 6ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0043 - accuracy: 0.9994\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0034 - accuracy: 0.9993\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0018 - accuracy: 0.9996\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0038 - accuracy: 0.9992\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0017 - accuracy: 0.9995\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0021 - accuracy: 0.9998\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0023 - accuracy: 0.9996\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 3s 30ms/step - loss: 0.0053 - accuracy: 0.9992 0s - loss: 0.0\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 2s 23ms/step - loss: 0.0025 - accuracy: 0.9995ETA: 2s - loss: 3.0206e\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0035 - accuracy: 0.9995\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 1s 9ms/step - loss: 0.0024 - accuracy: 0.9996\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 1s 8ms/step - loss: 0.0038 - accuracy: 0.9993\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0053 - accuracy: 0.9996\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0032 - accuracy: 0.9994\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0041 - accuracy: 0.9993\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0055 - accuracy: 0.9994\n",
      "Epoch 67/100\n",
      " 79/100 [======================>.......] - ETA: 0s - loss: 0.0037 - accuracy: 0.9994"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-943-7fdc69bfb884>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#m3.layers[2].set_weights([hold, bias])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mm7\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtinputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Public\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while(not(m7.evaluate(tinputs,labels)[0] == 0)):\n",
    "    hold = m7.get_weights()[10]\n",
    "    hold = hold * 1.01\n",
    "    bias = m7.get_weights()[11]\n",
    "    bias=bias * (1.01)\n",
    "        \n",
    "    m7.layers[6].set_weights([hold, bias])\n",
    "    #hold = m3.get_weights()[4]\n",
    "   # hold = hold * 1.01\n",
    "   # bias = m3.get_weights()[5]\n",
    "   # bias=bias * (1.01)\n",
    "    #m3.layers[2].set_weights([hold, bias])\n",
    "      \n",
    "    m7.fit(tinputs,labels,epochs = 100, batch_size=1000)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8d1f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "myInputs = tf.keras.Input(4,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afdfd113",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = tf.keras.layers.Dense(16, activation = \"relu\")(myInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab45e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2 = tf.keras.layers.Dense(16, activation = \"relu\")(layer_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "41c085af",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 = tf.keras.layers.Dense(1, activation = \"relu\", name = \"trump1\")(layer_1)\n",
    "out2 = tf.keras.layers.Dense(1, activation = \"relu\", name = \"trump2\")(layer_1)\n",
    "out3 = tf.keras.layers.Dense(1, activation = \"relu\", name = \"trump3\")(out1)\n",
    "out4 = tf.keras.layers.Dense(1, activation = \"relu\", name = \"trump4\")(out2)\n",
    "outFull= tf.keras.layers.Concatenate()([out3, out4, layer_2])\n",
    "outFull = tf.keras.layers.Dense(2, activation = \"softmax\")(outFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "65c671e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m8 = tf.keras.Model(inputs=myInputs, outputs= outFull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7692cc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcsAAAIjCAYAAABs/TA1AAAABmJLR0QA/wD/AP+gvaeTAAAgAElEQVR4nOzde1xUZf4H8M8BBteyNDXdzLTUvFFhlmVeMvKyag6aKXLxFl4aRFsx01LwEpZpoFm7UWA3jUuSpZD5q9RadAUtEzIVXM0G15LJiinTdIDn94fN7AADZ8CZeebyeb9e85I558x5vmfm8XxmznnmjCKEECAiIqK6zPaTXQEREZG7Y1gSERGpYFgSERGpYFgSERGpCJBdAHmWM2fOIC4uDpWVlbJLIbKpS5cueO6552SXQV6GnyypQXbt2oWsrCzZZRDZlJ2djZUrV8oug7wQP1lSo2zatEl2CUS1ZGRkICoqSnYZ5IX4yZKIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KcLiEhAQkJCbLLICJqNIYleT2j0QhFURr92IKCAqSlpSE0NLRR61AUxeZNhprPhTvVRuTO+OPP5HSJiYlS28/Ly2v0Y5OSkgAAK1asaPQ6hBAwGo1o0aIFAKC8vBzNmzdv9PquRM3nQggBg8GAtm3bApBbG5E7Y1iSVzMajUhLS2v0481BfyVhCaBaAMkKo7qeizZt2lj+ZlAS2cbDsORUBoMBWVlZlkOYNe/n5uZCURSEhoaitLTUskxubq5lmbS0NCiKgpiYGBw7dsyybluHDWtOS0pKQm5ubrV5jtbYc7Ke+FyYA9f8+ISEBBgMBiQnJ1drLzk52fIY63nW22WeHhoail27dtXaXqPRiJiYGJ7vJvcgiBogPT1dNKTbaLVaAcDyGOv7+fn5Qggh9Hq9ACB0Op0QQljmWy9TXl4udDqdACBKSkqEEEKUlZVVW7f1uqyn1bzfGPWtIz4+XsTHxzd4He70XNj7HJnbLSsrq1Vrfn5+tfvWtFqtKCsrs9Sq1WpFZmamEEKInTt3CgCisLCw1nNSWFhoc311aWj/JLJTLHsVNUhjdkb27LDtWaawsFAAEElJSVe8roZy1jrc5bmwd/vi4+OrhVfNxyUlJQkAQq/XV6vVHIxCCJGZmWmzTvMbDvM6y8vLVeupiWFJThLLw7DkMYKDgwEA8+fPl1yJfLKei8TERKSkpKC0tLTaoVazIUOGAAA+/vhjy7QdO3agX79+lvsZGRkAah8mrnlemOdPyZ0wLImoQdLS0jB79mxotdpa84KDg6HT6TBz5kwYjUYYjUYcP34cHTp0sCxjPm8qhKh1I3JXDEvyODqdTnYJbsNVz0VMTAwAICsrCzNnzsQ//vEPdO3atd6atm/fjry8PEyZMsXmctYDlIjcHcOSPIZ55zpy5EjJlcjnyueioKAAgwYNAgBEREQAQLVPijWZP11GREQgLS0Nffv2rTY/NTUVALBx40YYjUYA/xsdS+SuGJbkVAaDodrf1vfNO0rzvzWXBy5/kjEvs3HjRmi12mqH/8yfYszhUVBQYJln/jRkXr6xO2Tr+qz/NrPnqyO21uEuz0XNdqwVFBTgvvvuQ48ePao9vrS0tNonw5rrMH+atHWodvTo0QAun6Ns0aIFFEVB27ZtMX78+HprIZJK6vgi8jgNHW0Iq68+2LrZWsZ6mvXXCVJTU2uNkNTr9Zb5OTk5Qghh+VqC+asK5pGj8fHxlmlXWr81ta+OqD0HMp8Le2szt1Xz8ebRsdajX820Wq3lqy016fV6ER8fb/mqifnx1m1qtVrV16cmjoYlJ4lVhOBZdbJfRkYGoqKinD4YwzxCkt3TM58Lo9GIp556CikpKS5t11X9k3zObB6GJSKH27RpE8aPHy+7DCKHYViS26l5ntOXedJzkZCQUO2ydg8++KDskogchhdSJ7dj/gUM89+OPqRm7zVR3eFQnrOfC0cyj5BNTU3FjBkzJFdD5FgMS3I7zg4Edw6cmjyp1hkzZjAkyWvxMCwREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEK/uoINUpYWJjsEohqyc7Oll0CeSmGJTXIgw8+iPDwcFRWVsouxSsYDAYUFxfj/vvvl12KVxg/fjy6dOkiuwzyQorwpB/MI/IyGRkZiIqK8qjfrSTyQbN5zpKIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEgFw5KIiEhFgOwCiHzJ9OnT8eWXX6JFixYAgLNnzyIgIAAPPPCAZZnvv/8e69atw4gRIyRVSUQ1MSyJXOj111+3Of1f//pXtfsFBQUMSyI3wsOwRC60bNkyaDQa1eUmTJjggmqIyF4MSyIXCg8Ph8lkqneZoKAg9OzZ00UVEZE9GJZELtStWzfccccdUBTF5nyNRoOJEye6uCoiUsOwJHKxKVOmwN/f3+a8iooKREREuLgiIlLDsCRysQkTJqCysrLWdD8/P9xzzz3o2LGjhKqIqD4MSyIXu/HGG9GvXz/4+VX/76coCqZMmSKpKiKqD8OSSILJkyfbPG/5yCOPSKiGiNQwLIkkGDduXLWw9Pf3R0hICNq0aSOxKiKqC8OSSIKWLVti6NChloE+QghMnjxZclVEVBeGJZEkEydOhBACwOWvjIwZM0ZyRURUF4YlkSSjR49GYGAgAOChhx7CNddcI7kiIqoLrw1LbqWiogI5OTk2v1rhjTp16oQjR46gU6dOyM7Oll2OS7Rv3x733Xef7DKIGkQR5uNARG5gy5YtePjhh2WXQU7G3Q55mNn8ZElu5fz58wC4M/VWGRkZiIqKkl0GUYPxnCUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiUREZEKhiV5JYPBgKysLISGhsouhYi8AH/PkrzS0qVL8eqrr8ouo8EURalzXlJSErp27Yr7778fzZs3d2FVRMRPluSVUlJSZJfQKEIIlJWVWe6Xl5dDCAEhBIYMGYK0tDRMmjQJBoNBYpVEvodhSeRm2rRpY/nb+hNkcHAw1q9fDwCYPn06jEajy2sj8lUMS/IKRqMRWVlZUBQFoaGhOHbsmM3lDAYDkpOTLcvt2rXLMt36HGdubq5lmdLS0mrrMD8+LS0NBoOh1qHTutoAgISEBCQkJDR6O9u0aYO5c+ciNzcXeXl5brVtRF5NELmR9PR00ZhuqdVqhU6nE+Xl5UIIITIzMwWAausqKysTWq1WZGZmCiGE2LlzpwAgCgsLhVartSyfn58vhBBCr9cLAEKn01nWkZSUJPR6vRBCiPLychEfH293G0IIER8fL+Lj41W3p2bt1srLy2vV5Q7bZo/Gvr5EksWy15JbaczONCcnRwAQJSUllmnmQLFelzlArQGwhJetgKo5DYAoKyuz3C8rK2tQG/aqLyxtzfeUbWNYkoeK5WFY8ngfffQRAKBr166WabZGi2ZkZAC4POLUfAOAFStW2N2WTqdD27ZtkZWVBaPRiDZt2kAI4dA2GsObt43IHTAsyePZ+xWR3NxcALCMLrW+2SsuLg5arRYRERFo0aIFkpOTHd6GGvPAnvj4eIe26w7bRuSuGJbkc+oa/GOPrl27IicnB4WFhdDpdJg/f36tULnSNtQcOHAAABASEuLQdt1h24jcFcOSPF5qaioAoKioyK7lNm7caPl0Zh7daS9FUWA0GhEcHIyUlBQUFhZi/vz5Dm2jPgaDAS+++CK0Wi0efPBBh7Yre9uI3Jorz5ASqWnMABDzyE6tVmsZzWkeqQmrEZ/mASs1b3q9vto884ha60FC5oEv+HNAi7kdvV4vkpKSLLXU14YQ9o2GtW7XXIsQwjKyVavVVhuI4y7bZg8O8CEPxQE+5Pk6dOgAvV6PG2+8ER07dkRMTAxuu+02aLVaZGZmYvny5QAuf0dRr9dbzvXpdDro9Xp06NABbdu2tayvRYsW1f4FUG3+nDlzkJ2dDUVRkJ2djSeeeMIyr7427KEoSrV2W7RoYRlMs2PHDixatAg5OTnVLlzgKdtG5MkUIXh2ntxHRkYGoqKiOGjES/H1JQ81m58siYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVATILoDIluzsbIev8/z582jatCkURXH4uj1JRUUFAgLk/Nd3xutK5AoMS3IrXbp0AQCEhYVJroScJTAwUHYJRA3Gw7DkVu655x4IIRx2Ky4uxuDBg+Hn54cZM2bgp59+cuj6r/SWnp4OAC5r77fffsPIkSNx9dVXIzc3V8o2X7x4UXIvI2o4hiV5pQsXLmDJkiW444478NNPP2HPnj1ITU1Fy5YtZZcmVbNmzbB161ZMmDABY8aMQWpqquySiDwCD8OS19m2bRsef/xx/Pjjj1i1ahVmz54t7RydOwoICMDrr7+ODh06QKfT4dSpU3jmmWd8/lwuUX24ByGvcerUKcTFxWHz5s0YP348XnzxRbRr1052WW5r6dKluOmmm/DYY4/h1KlTSEtLg0ajkV0WkVviYVjyeBUVFUhKSkLPnj1RVFSEjz/+GJs2bWJQ2iE6Ohq5ubnYvHkzHnroIfz222+ySyJySwxL8mh79uxB7969kZCQgCeeeAKHDh3CsGHDZJflUYYPH47PP/8chw4dwqBBg/DDDz/ILonI7TAsySOdPXsW06ZNw/33348bbrgBhw4dwrJly/CXv/xFdmke6a677sLevXtx/vx53HfffThy5IjskojcCsOSPIoQAmlpaejevTv+7//+D1lZWfj4448t38+kxrvllluwd+9etG/fHgMGDMDu3btll0TkNhiW5DGKioowYMAAzJo1C5MmTUJxcTEvXuBgLVu2xKeffoqQkBAMGzaMV9wh+hPDktzeuXPnEBcXh7vvvhtCCHzxxRdYu3YtrrnmGtmleaWmTZti06ZNmDlzJsLDw/Hiiy/KLolIOn51hNxadnY24uLicOHCBfzzn//E9OnT4efH93jO5u/vj3Xr1qF9+/aYN28efv75ZzzzzDOyyyKShmFJbun48eOYPXs2PvnkE0yZMgUvvPACWrduLbssn/Pkk0+iZcuWeOyxx/DLL7/gpZde4sULyCcxLMmtXLx4Ec8//zxWrVqFzp0741//+hcGDhwouyyfNm3aNDRv3hxRUVEoLy/Hm2++ySsikc9hjye38emnnyI2NhY//PADli9fjri4OO6U3cS4ceNw7bXXYuzYsfj111/x7rvv8ms65FN48oek++GHHzBhwgQMGzYMt912Gw4fPownn3ySQelmhg0bhk8++QS7d+/m1X7I5zAsSZqKigqsW7cO3bt3xxdffIEPP/wQ77//Pjp06CC7NKpDv3798Nlnn+HIkSMYMmQIfvrpJ9klEbkEw5Kk2LdvH/r06YMFCxZgzpw5OHz4MB566CHZZZEdgoODkZeXB4PBgEGDBuH777+XXRKR0zEsyaV+/vlnPPbYY+jXrx9atmyJoqIirFixAk2bNpVdGjXArbfeit27d6OqqgoDBgzAiRMnZJdE5FQMS3IJIQTefvttdO/eHTk5OdiwYQN27tyJ7t27yy6NGql9+/bIy8tDq1atMHDgQBw6dEh2SUROw7Akpzt8+DAGDRqE6OhoTJgwAUePHkVUVJTsssgBWrdujZ07d6Jbt2544IEHUFBQILskIqdgWJLTnDt3DgsXLsSdd96JCxcuYP/+/Xj55ZfRokUL2aWRA1177bX46KOP0L9/fwwdOhQ7duyQXRKRwzEsySm2bt2KoKAgpKam4sUXX8S+fftw1113yS6LnKRp06bYvHkzRo8ejVGjRmHLli2ySyJyKIYlOdTJkyeh1Wrx8MMP4/7770dxcTFmzZrF67n6AI1Ggw0bNiA6OhphYWH8xRLyKvzWNznEpUuXkJycjMTERHTs2BE7d+5ESEiI7LLIxfz8/PDPf/4TGo0GERERqKioQEREhOyyiK4Yw5Ku2K5duxAbGwu9Xo+EhAQ88cQTCAwMlF0WSaIoCtatW4fAwEBMmjQJFRUVmDRpkuyyiK4Iw5Ia7cyZM3jyySeRnp6Ohx56CNu3b8fNN98suyxyEy+88AICAgLw6KOPoqKiAo8++qjskogajWFJDVZVVYVXX30VixcvxrXXXosPPvgAo0ePll0WuaGVK1ciICAA06ZNQ0VFBWbMmCG7JKJGYVhSg3z55ZeIiYlBUVER4uLisGTJElx99dWyyyI3lpiYiMDAQDz22GMwmUyYNWuW7JKIGoxhSXYpLy9HQkICXnnlFQwYMAAHDx5EUFCQ7LLIQyQkJECj0WD27Nnw8/ODTqeTXRJRgzAsSdU777yDJ598EpWVlXjjjTcwefJkKIoiuyzyME899RSqqqowa9YsaDQaTJs2TXZJRHZjWFKdiouLERsbi88//xzTp0/H888/j+uuu052WeTBFi1aBJPJhJkzZ8Lf3x9Tp06VXRKRXRiWVMuFCxewYsUKJCUlISgoCHv37sW9994ruyzyEkuXLkVFRQWmT58OjUbD6wSTR2BYUjUffvghHn/8cfz0009YvXo1YmNjERDAbkKOlZiYiIsXL2LKlCkIDAzE+PHjZZdEVC/uBQkAUFpairlz5+KDDz7AhAkTsHbtWtxwww2yyyIvtnr1aphMJkRFRcHf3x9jx46VXRJRnRiWPq6iogJr1qxBYmIibrjhBnzyyScYOnSo7LK81s6dO6v9UPL+/fsBAKmpqdWWGz58ODp06ODS2mRYs2aN5ZJ4mzdvxqhRo2SXRGSTIoQQsosgOXbv3o1Zs2bhxIkTeOqpp7Bw4UI0adJEdllezTyKWKPRALj8o9hCiGoXmjeZTFiwYAFWrVolpUZXE0LgscceQ3p6OrZt24YHHnhAdklENc3mT0H4oLNnz2Lq1KkYNGgQ2rdvj6+//hpLlixhULpAdHQ0NBoNTCYTTCYTKioqUFlZablvMpkAwKcuQq8oClJSUqDVajFmzBh8+eWXsksiqoVh6UWMRiMURcGUKVNszq+qqkJqaiq6deuGHTt24N1338X27dvRpUsXF1fquyIiIiyBWJfrrrsOQ4YMcVFF7sHf3x8bNmxAv379MHLkSBw5ckR2SUTVMCy9RGVlJcaNGwcA2LBhA/bs2VNtfmFhIfr374/Y2FhMmTIFxcXFHIEoQUhICFq1alXnfI1Gg/DwcJ8cgRwYGIj33nsP3bt3x9/+9jd89913sksismBYeokFCxbgs88+A3D5XfqMGTNgMpnw22+/Ye7cuejTpw/8/Pzw5ZdfYs2aNWjWrJnkin2Tv78/Jk6cWOdPmJlMJkRGRrq4Kvdx1VVXITc3F9dffz2GDh2KM2fOyC6JCAAH+HiFDRs21Dr06u/vj1mzZmHz5s24ePEinn/+eUybNo2XqXMD+/fvr/MiD+3atcN///tfn3+dfvzxRwwYMABNmjRBXl4eWrRoIbsk8m0c4OPp9u3bh+nTp9eaXllZibS0NAwbNgzFxcWYPn26z++A3UWfPn3Qvn37WtM1Gg2vu/un66+/Hjt37sQvv/yC4cOH4/z587JLIh/HsPRgp0+fRmhoKKqqqmzOr6ysxC+//ILWrVu7uDKqj3kQlvnrI2Ymkwnh4eGSqnI/7du3x0cffYTi4mJERESgsrJSdknkwxiWHuqPP/5AaGgofvnllzp3IiaTCVu3bsVHH33k4upITWRkZK1RsV26dEFwcLCkitzT7bffjq1bt2LHjh2YM2eO7HLIhzEsPdSMGTPw9ddfq34Nwc/PD1OnTsXvv//uosrIHj179kSPHj0s9zUaDX+Bow6DBg1CZmYmUlNT8fzzz8suh3wUw9IDJScnIz09HRUVFfUupygKhBD48ccf8c4777ioOrLX5MmTLYdizZd8I9tCQ0Px8ssvY9GiRUhPT5ddDvkgjob1MB9//DFGjhxp8zxlYGAgKioqUFVVhRYtWqBv377o168f+vXrh8GDB0uoluqj1+txyy23QAiBO++8E1999ZXsktze008/jTVr1mD79u148MEHZZdDvmO2733z2YN98803GD58OIDLXw3x8/ODyWSCRqPBbbfdhoEDB+Lee+/Fvffei86dO0uultR07NgRwcHBKCwsrPOqS1Tdc889h1OnTmHs2LHYvXs3br/9dtklka8QNezbt08A4I03AUAsXry4ZhdxmMWLF0vfPt7c57Zv3z67+s3FixfF/fffLzp37iwMBoPT+ieRldhanyyPHz8OANi0aVPNWSSZyWTCr7/+Wu/l0hwpKioKJ0+edNr6T548CY1G49PnoCorK2EwGHz+t0PDwsJw/Phx3HPPParLmi+L17dvX4SFheHjjz+u84pIRI5S52FYXjeUtmzZ4vQ2xo8fz75GDXb99ddjy5YtGDhwIGbPnl3r90CJHI2jYYnII91+++3YsGED3njjDbz00kuyyyEvx7AkIo8VGhqKZ599FvPmzcMnn3wiuxzyYgxLIvJoCxcuRGRkJMLCwlBSUiK7HPJSDEsi8njr169H9+7dMWrUKJSXl8suh7wQw5KIPF5gYCBycnLwxx9/4JFHHuFF18nhGJZE5BXatGmDDz/8EAUFBXj88cdll0NehmFJRF4jODgYGzduREpKClJSUmSXQ16EYUlEXmXs2LF45plnMHfuXOzZs0d2OeQlGJZE5HUWL16MUaNGISwsDGfOnJFdDnkBhiUReR1FUfDGG2/gmmuuQVhYmOrvvhKpYVgSkVdq3rw53n//fRw4cAALFy6UXQ55OIYlEXmtoKAgvP7661i7di1/HIKuCH/Pkoi8Wnh4OAoKCjBt2jTcdttt6Nmzp+ySyAPxkyUReb3Vq1ejV69eeOSRR/Drr7/KLoc8EMOSiLxeYGAg3n33XZSXlyM6OhpCCNklkYdxSFgajUYoiuKIVbmE0WhEQUEB0tLSEBoa2qh1KIpS5y05ORm5ubkwGo0Orpw8ra+VlpYiJiYGiqIgJiYGu3btavA62Ncco127dti0aRO2bt2K5ORk2eWQh3FIWObl5TliNS6TlJSEbdu2YebMmcjNzW3UOoQQKCsrs9wvLy+HEAJCCAwZMgRpaWmYNGkSDAaDo8omeFZfMxqNKCoqQkpKCsrLyzFo0CAMHjy4wX2Ofc1xBg4ciNWrV+Opp57C559/Lrsc8iBXHJZGoxFpaWmOqMVlEhMTkZiYeMXradOmjeXv5s2bW/4ODg7G+vXrAQDTp0/nu34H8bS+lpeXB61WC+By/wgPDweARh3NYF9znLlz52Ls2LGYMGEC/vvf/8ouhzzEFYdlUlKS5Z2y+dCQwWBAbm4uQkNDYTQaERMTg4SEhGqHj8xqTjMYDMjKyrLsUHJzcy2HsEpLSwEAWVlZtaZZtwkAaWlplmWOHTvW4O1KSEhAQkJCo5+XNm3aYO7cucjNza31achgMCA5ORmKoiA0NNRyaK6ubQ8NDbVsp5n58WlpaTAYDLUOTdbVhifztL5mDsqadDpdtfvsa65lvmBBq1atEBERwV8oIfuIGtLT04WNyfUCUO0xWq3WMi0/P18UFhYKnU4nysrKai2r1+urTbN+bGFhoRBCiPz8fAFA6HQ6kZ+fX+1xOp2uWg3mNoUQory8XOh0OgFAlJSUqNZtLT4+XsTHxzd4262Vl5dXq1EIIcrKyoRWqxWZmZlCCCF27txp2daaz5ut7RRCiKSkJKHX6y1txMfHV6uhvjYaIjIyUkRGRjboMc5ev6f2NfMyAEROTk616exrl7ctPT29QY+5Ut9884246qqrxKJFi1zaLnmkWKeEpfW08vJyu5dtyDL2Pq6wsFAAEElJSXbV3VBq66g5PzMz0+Z2mHeW9m5nWVmZ5b45GOxtw16eEJbW09y5rwlxOUi0Wm2tOu3lzX1NRlgKIURaWprw8/MTn376qcvbJo/i/LBszLKO3IE1ZnpDNHQHZv2OvuatrvXVnGb+BJOZmWlzx6vWhr08LSwbs6yr+poQl18X86e4xvDmviYrLIUQIiIiQrRt21b88MMPUtonjxDL71k6kXmwRXx8vGWa+Zyb+HM0o/XNXnFxcdBqtYiIiECLFi1qDYN3RBvkWFlZWdBqtejbt69T1s++1nivvfYarr32WkyaNAlVVVWyyyE35TNhWXNQhSscOHAAABASElJrXmMGHZl17doVOTk5KCwshE6nw/z5821+b+xK2qDGq9nXioqKcPjwYcyYMcNpbbKvNd4111yDrKws7N69GytXrpRdDrkprw9L83/ikSNHurRdg8GAF198EVqtFg8++KBlempqKgBg48aNlk8D5tGE9lIUBUajEcHBwUhJSUFhYSHmz5/v0Dao4Wz1NYPBgB07dlT7qlJRURFiYmIc1i772pXr3bs3kpKSsHTpUv5gNNlW88BsY85Zms9blJWViaSkJJsjEc1qjhg0jz4EUGsUo/kcifU082ADW9PM980j88yj97Raba06zKMHrduxZs8IxbrWYR5tqNVqqw2OqFm39U2v19vcdus2rLczPj7eMkpRr9dXG1RSXxsN4Y7nLD2pr5lHitp6LaxHxLKvyT1naVZVVSUefvhh0b59e3H27FmptZDbccwAH/MowPj4+Fr/eWoGlV6vt+xAzLY1erEAACAASURBVDsM89BzW//xhBANnmY9ND41NbXOUZK21mGmtgOrax3A5dGQ9Q3k0Ov1liH4Op3OsmNpyHaaw8Lcnr1tNIQ7hqUn9TVzWNu6WX+9hH3NPcJSCCF+/vln0bFjRzFq1ChRVVUluxxyH7GKENXPxGdkZCAqKsqjTtCbmb8s7Ym1u6OoqCgAQHp6ukeu35nY1xxLURSkp6cjMjJSdinIz8/HoEGD8Pzzz2PevHmyyyH3MNvrz1kSETXEfffdh8TERDz99NPYv3+/7HLITXhNWFpfRJoXlCZnYl/zfgsWLMDgwYMRERGB3377TXY55Aa8Jizbtm1r828iR2Nf836KouDNN9/E77//jjlz5sguh9yA14Sl8NAvRJPnYV/zDW3btsUbb7yBDRs24N1335VdDknmNWFJRORoI0eORGxsLGJiYnDq1CnZ5ZBEDEsionqsXr0aN9xwA6ZMmcLL4fkwhiURUT2aNm2K9PR0/Pvf/0ZSUpLsckgShiURkYpevXrhueeeQ0JCguU6vORbGJZERHaYN28eBg4ciIkTJ+L8+fOyyyEXY1gSEdlBURS8/fbbMBgM1S4mT76BYUlEZKcbb7wRr732Gl599VXLb3mSb2BYEhE1wLhx4/Doo49i2rRpOHPmjOxyyEUYlkREDbRu3To0b94c0dHRvDCFj2BYEhE1ULNmzfDOO+/gk08+QVpamuxyyAUYlkREjXDvvfdi4cKFeOKJJ3Dy5EnZ5ZCTMSyJiBpp6dKl6NSpE6ZOncqr+3g5hiURUSMFBgZiw4YNKCgowLp162SXQ04UUHPCVVddBeB/vwRPvu3RRx912rqbNGmCN998ExkZGU5rgzyHed/jaYKDg7FkyRIsWrQIw4cPR48ePWSXRE6giBpDuSoqKpCTk4PKykpZNbmFsLAwPP744xgwYIDsUqTq27cvbrrpJqes+9SpUygoKHDKuj3Fnj178NJLL2HTpk2yS5HK398foaGhCAio9f7dI1RWVqJ///6oqqrC3r17PXY7qE6za4UlXaYoCtLT0xEZGSm7FPJiGRkZiIqK4tcPvEBJSQnuvPNOPP3000hISJBdDjnWbJ6zJCJygG7dumHlypVITEzEwYMHZZdDDsawJCJyEPOpm8mTJ+PixYuyyyEHYlgSETmIoih44403oNfrsWTJEtnlkAMxLImIHOjmm2/G2rVrkZycjL1798ouhxyEYUlE5GDTpk3DsGHDMHXqVPzxxx+yyyEHYFgSETlBamoqDAYDli5dKrsUcgCGJRGRE7Rv3x7JyclITk7G/v37ZZdDV4hhSUTkJNHR0Rg8eDCio6M5OtbDMSyJiJxEURSkpaWhtLQUzzzzjOxy6AowLImInKhDhw5YtWoVVq9ejQMHDsguhxqJYUlE5GQ6nQ79+/dHdHQ0TCaT7HKoERiWREROpigKXn/9dZw4cQIrV66UXQ41AsOSiMgFOnfujGeffRbPPvssDh06JLscaiCGJRGRi8yZMwf33HMPHn30UVRUVMguhxqAYUlE5CJ+fn544403cOTIEaxevVp2OdQADEsiIhe69dZbsXz5ciQmJuLYsWOyyyE7MSyJiFwsLi4OPXv2xIwZM/jD3x6CYUlE5GIBAQFIS0vD3r17sX79etnlkB0YlkREEvTu3RtxcXFYsGABvv/+e9nlkAqGJRGRJMuXL0erVq3w+OOPyy6FVDAsiYgkadq0KV599VW8//772LJli+xyqB4MSyIiiYYMGYIpU6YgNjYWRqNRdjlUB4YlEZFkycnJqKiowFNPPSW7FKoDw5KISLKWLVti3bp1eO2117B7927Z5ZANDEsiIjcQHh6OkSNHYubMmfyhaDfEsCQichMpKSk4ffo0nnvuOdmlUA0MSyIiN3HTTTchMTERq1atQklJiexyyArDkojIjcyePRtBQUHQ6XS8FJ4bYVgSEbkRf39/pKamYvfu3diwYYPscuhPDEsiIjdz1113ITY2FvPnz8dPP/0kuxwCw5KIyC2tWLECTZo0wZNPPim7FALDkojILV1zzTV46aWX8NZbb+Ff//qX7HJ8XoDsAtzFL7/8Umva77//Xm361VdfjcDAQFeWRV7m0qVL+P333y33zX/X7H/XXXedS+si9zR27FiMGjUKMTExOHjwIJo0aQIAKCsrQ2JiIh5++GEMHjxYcpU+QpBYuHChAKB6CwwMlF0qeTh7+hkAkZiYKLtUchPfffedaNasmUhMTBSVlZXi5ZdfFs2aNRMAxEMPPSS7PF8Ry8OwADp16mTXcrfeequTKyFvFxQUZNdybdq0cXIl5Ck6duyIpUuXIjExET179sTf//53nDt3DgDw9ddfS67OdzAsAYwbNw4BAfUfkfb398e8efNcVBF5q3nz5sHf37/eZQICAjBu3DgXVUTu7uzZszh69ChMJhNOnDiBqqoqy7xTp07h7NmzEqvzHQxLXL6I8dChQ+vdifn5+WHs2LEurIq80dixY+HnV/d/O39/fwwdOhQtW7Z0YVXkjqqqqrB+/Xrceuut2LhxI4QQqKioqLXcF198IaE638Ow/NPEiRPrvFpGQEAARowYgRYtWri4KvI2LVq0wIgRI+o8kiGEwMSJE11cFbmj6OhozJgxA+Xl5TCZTDaXCQwMZFi6CMPyT6NHj65zpGtlZSUmTZrk4orIW02aNAmVlZU25wUGBmL06NEurojc0ZAhQwCg3iNeJpMJe/fudVVJPo1h+aerr74aY8aMgUajqTXvL3/5Cx566CEJVZE3euihh/CXv/yl1nSNRoMxY8bg6quvllAVuZuJEyfiwIEDaN26tc39EnD5SMS+fftcXJlvYlhaiYqKqnW4Q6PR4JFHHkHTpk0lVUXepmnTpnjkkUdq7QBNJhOioqIkVUXuqHfv3igsLESvXr3qPHRfXl6OkydPurgy38OwtDJs2DBce+211aZxB0bOYOuN2bXXXothw4ZJqojc1V//+lfs3r0b4eHhUBSl1nxFUbB//34JlfkWhqWVwMBATJgwodo7/uuuu85y7oDIUYYMGVLtKj0ajQYTJkzgFaLIpiZNmmDjxo1YvXo1/Pz8qo2o1mg0DEsXYFjWYP2OX6PRIDw8XPU7mEQNFRAQgPDwcMsbMx7BIHvMnz8fubm5aNq0qWW/dOnSJfz73/+WXJn3U0Rd35fwUVVVVWjXrh3KysoAALt378aAAQMkV0XeaM+ePRg4cCAAoG3btvj+++/r/Q4mkdnRo0cxYsQIfP/99zCZTGjSpAnOnTvHN/bOM5v/M2vw8/OzvMNv164d+vfvL7ki8lb9+/dHu3btAFw+osGgJHv16NEDX331leWN/MWLF3H48GHJVXm3Wm9Dzpw5g7i4uDq/B+YLzL8AUVVVhQkTJkiuRq5JkyZBq9U6Zd25ubnYuHGjU9btKcwB+fXXXyMsLExyNfL4+/tj7dq1+Otf/+qU9S9atAjHjx93yrplatWqFTp37owTJ04gOjoanTt3ll2Sx+vSpQuee+65WtNrvZXdtWsXsrKyXFKUu7ruuutw2223ITg4WHYpUmVnZzu1L2RlZSE7O9tp6/cEvXr1Qrdu3Xz+J7mysrKwa9cup61/5cqVXtnXFEVB7969ERISgptuukl2OR4vOzsbK1eutDmvzgPcmzZtclpB5BlcMeAkMjIS6enpTm+H3Jutr0Q4Wnp6OiIjI53eDnmujIyMOvd7PElCRESkgmFJRESkgmFJRESkgmFJRESkgmFJRESkgmFJRESkgmFJRESkgmFJRESkgmFJRESkgmFJRESkgmFJRESkgmFJRESkgmFJRESkgmFJRESkgmFJRGQHg8GArKwshIaGyi6FJHBIWBqNRpf8Hp2jGAwGJCQkQFEUKIrSqB84Nj/W1i05ORm5ubkwGo1OqN63eVpfqyktLa3B9bOvuYelS5ciIiICubm5sktpFKPRiIKCAqSlpTU68H25LzokLPPy8hyxGpcwGAz49ttvkZiYCCEEMjMzERERgeTk5AatRwiBsrIyy/3y8nIIISCEwJAhQ5CWloZJkybBYDA4ehN8mif1tZqKioowc+bMBj+Ofc09pKSkyC7hiiQlJWHbtm2YOXNmowPfl/viFYel0WhEWlqaI2pxiW+//RZ9+/a13A8PDwcAzJ8/v8HratOmjeXv5s2bW/4ODg7G+vXrAQDTp0/32ndaruZpfc2a0WjEe++91+jHs6/RlUpMTERiYuIVr8dX++IVh2VSUpLlXYr547jBYEBubi5CQ0NhNBoRExNT7bCn9WGomtNqnhfIzc2FoiiIiYlBaWkpACArK6vWNOs2gf8d7oqJicGxY8cs7VkHJQDLCxofH19tekJCAhISEhr9vLRp0wZz585Fbm5urU9DBoMBycnJUBQFoaGh2LVrV73bHhoaatlOM/Pj09LSYDAYah3aq6sNT+Zpfc3a+vXrMWfOHJvz2Nfck9FotLz+oaGhdb623vQcsy/WQ9SQnp4ubEyuF4Bqj9FqtZZp+fn5orCwUOh0OlFWVlZrWb1eX22a9WMLCwuFEELk5+cLAEKn04n8/Pxqj9PpdNVqMLcphBDl5eVCp9MJAKKkpKRW3Xq9XsTHx9ucHx8fL+Lj4xu87dbKy8ur1SiEEGVlZUKr1YrMzEwhhBA7d+60bGvN583WdgohRFJSktDr9ZY2zNtgTxsNERkZKSIjIxv0GGev3xP72s6dOy3L2eov7GuXty09Pb1Bj3H2+rVardDpdKK8vFwIIURmZmat18CTnmOz+vqRr/fFevIv1ilhaT3N3NHsWbYhy9j7uMLCQgFAJCUlVZtuveO0Nd9e9XUaW/PN/+FqLmPuoPZuZ1lZmeW+ORjsbcNenhCW1tPcsa+VlZWJ1NTUeh9nL2/ua+4Wljk5ObXe+JhDwFOf47rad8Y6PPV5khqWjVnWkTuw+qYLcXkHZ36HYr1Ts1dDO431u6iat7rWV3Oa+RNMZmZmrYCwpw17eVpYNmZZZ/e1mn3KlWHpSX3N3cLSvN221uOpz3Fd7TtjHZ76PDEsVZ6wkpKSRneg+h5nfidq/c6moZ3M1rSSkpJqHaPmp2JH/GcQgmHZ0MfVnJ6Tk2M5bKT2OHt4c19zt7C0t0950nPsyPV5a1+sLyx95qIEOp2uznldu3Z1SpsHDhwAAISEhNSaV9dgAXt07doVOTk5KCwshE6nw/z5821+9eVK2qDGM/e10NBQdOzYsc7BRo7EviYXn+P/8da+6PVhaX7iRo4cWecy5hGxmZmZDmvXYDDgxRdfhFarxYMPPmiZnpqaCgDYuHGjpV3zCC57KYoCo9GI4OBgpKSkoLCwsNpXXxzRBjVczb4mhKh1M7P++0qxrzmeebuKiorsWo7P8WVe3Rcb8DG0TuaPxmVlZSIpKcnmSESzmiMGzaMPAdQaxWg+Lm09zXyC19Y0833zaCjziCmtVlutVlujqmqeBLZnVJj1CX/rY+jmEV5arbbaCemadVvf9Hq9zW23bsN6O+Pj4y3boNfraw0qqauNhnDHw7Ce1NdssVUr+5r7HYY1DwDUarWWbTGPrjT3HyE86zmu2b6t836+3hedfs7SPAowPj6+VsE1dx56vd6yw8vJyRFCCMtwX1sbK4Ro8DTr4cipqanVXlDzKDfzLSkpyTJc2Zpap7H1oqit0/o5MA8q0ul0lhezIdtpDgtze/a20RDuGJae1NdsaUxY+kJfc7ewFOLydpnfcJnfXFn3H+vlPOU5tnWz5ut9sb6wVP4swiIjIwNRUVEOPUzkKubzQJ5YuzuKiooCAKSnp3vk+p2Jfc2xFEVBeno6IiMjPXL95B3qyb/ZXn/OkoiI6Ep5TVhaX7jXGy/iS+6DfY3I9wTILsBR2rZtW+1vHh4jZ2FfI3dh71eQ2EevnNeEJTsDuQr7GrkL9kXX8ZrDsERERM7CsCQiIlLBsCQiIlLBsCQiIlLBsCQiIlLBsCQiIlLBsCQiIlLBsCQiIlLBsCQiIlLBsCQiIlLBsCQiIlLBsCQiIlLBsCQiIlJR56+OhIWFubIOsoPJZMLZs2dxww03uKS97Oxsp/+yfEZGBkwmk1PbIAKAqKgobNmyxaVtVlVV4fTp07jppptc2i41TnZ2dp3z/JctW7bMesL111+P06dP86df3FBpaSkKCgpgNBrRqlUraDQap7YXFBSEqKgodOvWzSnrDwwMREVFhVPW7SkMBgMOHDiAjh07yi5FqjvuuAMxMTFo1qyZU9Z/6dIll73JNDtz5gz27NkDvV6PDh06IDAw0KXtU8MFBQVh9OjRGDx4cM1ZHymCqehRPv30U8TGxuKHH37AkiVLEBcXh4AAr/lZUp+TkZGBqKgovjn1Inq9HnFxcfjggw8wduxYrF27Fh06dJBdFl2Z2Txn6WGGDh2KQ4cOYf78+Vi6dCnuvPNO7N69W3ZZRD7vwoULSExMRI8ePXDkyBF8/PHH2Lx5M4PSSzAsPVCTJk2wdOlSfP3112jfvj0GDRqERx99FGfPnpVdGpFP+vDDD3HHHXdg9erVlv+bw4YNk10WORDD0oN16dIF27dvx7vvvotPP/0U3bp1Q2pqKqqqqmSXRuQTTpw4gdDQUGi1Wtx11104evQoFi5cyPOTXohh6QXGjx+P4uJiTJkyBbGxsRgwYAAKCwtll0XktS5cuIClS5fi9ttvx4kTJ7Br1y5kZWWhffv2sksjJ2FYeolmzZphzZo1+PLLL6EoCvr06YO5c+fit99+k10akVfZsmULevbsibVr1yIxMRFFRUUICQmRXRY5GcPSywQHB2PPnj145ZVX8M4776B79+7YtGmT7LKIPN7x48cxYsQIjB07Fv369UNJSQmeeOIJjkb3EQxLL6QoCmbMmIHi4mIMHz4c4eHhGDZsGI4fPy67NCKP8/vvv2Px4sUICgrC6dOn8dlnnyE9Pd3l39skuRiWXqx169Z4/fXXkZeXhzNnzuD222/HsmXLcOHCBdmlEXmE9957Dz179sQrr7yC1atX46uvvsKgQYNkl0USMCx9wIABA/DVV19hxYoVSE5Oxh133IFPPvlEdllEbuvo0aMYOnQowsLCEBISgqNHj+Lvf/87D7n6MIaljwgICMATTzyBI0eOIDg4GH/7298QFhaG06dPyy6NyG2cO3cOCxYsQK9evfDTTz9hz549eOutt/DXv/5VdmkkGcPSx9x000147733sG3bNhw4cAA9evTAiy++6PPXaCXKzMxEt27dkJaWhrVr1+KLL75Av379ZJdFboJh6aNGjhyJb775BnPnzsXChQtx9913Iz8/X3ZZRC53+PBhhISEYOLEiRgxYgT+85//YNasWfD395ddGrkRhqUPa9q0KZ555hkcOnQIrVu3xoABAzBz5kz89NNPsksjcjqj0Yh58+ahV69eOHfuHPbu3Yv169ejdevWsksjN8SwJHTt2hU7duzAO++8gw8//BDdu3fHW2+9xV/CIK8khMDGjRvRo0cPbNy4Ef/85z+xb98+3HvvvbJLIzfGsCSLiIgIHD16FBEREZg+fTruv/9+HDp0SHZZRA5TVFSE+++/H1OnTsXo0aNRXFyMmTNnws+Pu0KqH3sIVdO8eXO89NJL2L9/Py5duoTevXtjwYIFOHfunOzSiBqtvLwcjz/+OO6++25UVFTgiy++QEpKClq1aiW7NPIQDEuyqXfv3sjPz8fLL7+MtLQ09OzZE++//77ssogaRAiBN998E926dUNWVhZee+017N27F71795ZdGnkYhiXVyc/PDzqdDiUlJXjggQcwbtw4jBo1Ct9++63s0ohUHTx4EP369cPMmTMRFhaGY8eOITo6GoqiyC6NPBDDklS1adMGGzZswGeffYaTJ0/itttuw4oVK3Dp0iXZpRHV8vPPP2PWrFm4++674e/vj/379+Pll19GixYtZJdGHoxhSXYbNGgQDh48iCVLlmDlypW44447sHPnTtllEQEAqqqqkJaWhm7dumHLli148803sXv3btx5552ySyMvwLCkBgkMDMRTTz2FI0eOoFu3bhgyZAiioqLwww8/yC6NfNj+/fvRt29fzJo1CxMnTsSRI0cwefJkHnIlh2FYUqN07NgRW7duxdatW/Hvf/8bPXv2xMsvv4zKykrZpZEPOXv2LGbMmIH77rsPV199NQ4ePIi1a9fykCs5HMOSrkhoaCiOHDkCnU6H+fPn495778UXX3whuyzyclVVVXjllVfQtWtXbNu2De+88w527dqF2267TXZp5KUYlnTFrrrqKqxcuRIHDx5Es2bNLIfDysvLZZdGXig/Px9333034uLiMG3aNBw7dgwRERE85EpOxbAkh+nZsyc+++wzvPXWW9i8eTO6d++OjRs38rJ55BBlZWV49NFH0b9/f7Rs2RIHDx7ECy+8gGbNmskujXwAw5IcSlEUTJo0CSUlJXj44YcxdepUPPjggzh69Kjs0shDVVRU4KWXXkL37t2xa9cuvPvuu9ixYwd69uwpuzTyIQxLcooWLVogJSUFBQUF+PXXX9GrVy88/fTTOH/+vOzSyIPk5eXhrrvuwpNPPomYmBgcPnwY48ePl10W+SCGJTlVnz59sH//fiQlJeHVV19FUFAQcnNzZZdFbu6HH37ApEmT8MADD6Bdu3b45ptv8Nxzz/GQK0nDsCSn8/f3x5w5c3DkyBHcd999CA0NxejRo6HX62WXRm6moqICa9asQffu3bF7925s3rwZ27dvx6233iq7NPJxDEtymRtuuAEZGRnYsWMHiouLERQUhFWrVvGyeQQA+PzzzxEcHIzFixfj73//O44ePYqHH35YdllEABiWJMHgwYNx6NAhLFy4EMuXL8edd96JvLw82WWRJKdPn0ZERARCQkJwyy234NChQ3jmmWfQtGlT2aURWTAsSYrAwEAkJCTgm2++wc0334wHHngAU6ZMgcFgkF0aucilS5ewevVqdO/eHV988QVycnLw4YcfokuXLrJLI6qFYUlSderUCdu2bUN2djY+++wzdOvWDa+99hqqqqpkl0ZO9OmnnyI4OBjLli3Dk08+iUOHDkGr1coui6hOiuA3xslNnDt3DsuWLcO6devQu3dvpKSkeN2P9E6fPh1ffvml5dqlZ8+eRUlJCfr3729Z5vvvv8e6deswYsQIWWU6TWlpKebNm4fNmzdjzJgxWLt2LW6++WbZZRGpmc2wJLdz6NAhzJo1C/n5+YiNjcUzzzyD5s2byy7LIey9JNuSJUuwfPlyJ1fjOpcuXcILL7yA5557Du3atcPLL7+M4cOHyy6LyF6zeRiW3M7tt9+OvLw8pKamIiMjAz169EBWVla9j/nuu+9w5MgRF1XYeMuWLYNGo1FdbsKECS6o5sp9++23qpcz3L59O4KCgrBy5UosXrwYhw8fZlCSx2FYkltSFAXR0dEoLi7GqFGjEBkZiaFDh+LYsWO1lq2srETPnj0RFBSEffv2SajWfuHh4TCZTPUuExQU5BGXcktPT0fnzp3x9NNP25x/8uRJjBkzBiNHjkSvXr1w5MgRLFq0CIGBgS6ulOjKMSzJrbVq1QqpqanYs2cPfvzxR9x+++1YunQpLly4YFnmH//4B/744w8oigKtVov//ve/EiuuX7du3XDHHXfUeThWo9Fg4sSJLq6q4QoKCjB16lQAwJo1a3Dy5EnLvAsXLmD58uUICgrCsWPH8OmnnyI7OxsdOnSQVC2RAwgiD2EymcTatWtFs2bNROfOncW2bdvE6dOnxVVXXSUACABCo9GI3r17i/Pnz8sut07JyckiICDAUrP1TVEU8d1338kusV56vV5cd911wt/f3/KcDx8+XAghxNatW0WnTp1Es2bNxOrVq8XFixclV0vkELEc4EMe5/Tp04iLi0N2djYGDhyIgoKCaoc2AwICMGHCBLzzzjsSq6zb6dOncdNNN9U61+fn54c+ffqgoKBAUmXqzp07hz59+uDEiRO1Difffffd+PLLLxEREYGkpCS0a9dOUpVEDscBPuR5brzxRmzatAlr1qzB7t27a+20KyoqkJGRgaSkJEkV1u/GG29Ev3794OdX/b+foiiYMmWKpKrUVVZWIjw8HMePH6/1nPv5+aGkpAQff/wxMjIyGJTkdRiW5JEuXryIl156Cf7+/jbnCyGwcOFC/N///Z+LK7PP5MmTbZ63fOSRRyRUY5/Fixdj+/btqKioqDWvqqoKFy5ccPsBVkSNxbAkj7Rq1SqcOnUKlZWV9S43btw4myNoZRs3bly1sPT390dISAjatGkjsaq6vf3221i1alW9V1aqqKjAs88+y1+TIa/EsCSPc+LECSxdulQ1KKuqqnDp0iWMGDECRqPRRdXZp2XLlhg6dKjlk7EQApMnT5ZclW179uzB9OnT7Vr20qVL/A4leSWGJXmcM2fOWP728/Or90v+JpMJp06dQlhYmGq4utrEiRMtg3w0Gg3GjBkjuaLavv32W2i1WtULDwQEBECj0UAIgeLiYl7bl7wOR8OSRxJCoKSkBPv27cP+/fuRl5eHo0ePorKyEhqNBlVVVdXC0d/fH/PmzcPq1aslVl3d77//jtatW+OPP/7A2LFjsXnzZtklVWM0GtG3b99aI1/9/f3h5+cHk8kERVHQqVMn3HfffejTpw/69OmD4OBgXHXVVRIrJ3I4XhvWE506dcqtv14gy6VLl/Dtt9/iP//5D/7zn/+guLgY5eXl1ZZ5+OGHERERIanC2pYtW4YjR45g/vz5uOeee2SXU83EiRNr/TB369at0bVrV3Tp0gWdOnXCLbfcwt+dtOLv74/Q0FAEBATILoUci2HpiaKjo/Hmm2/KLoOIbPjggw/c8pA6XZHZfPvjgS5evIjIyEikp6fLLoWIrCiKgvPnz8sug5yAA3yIiIhUMCyJiIhUMCyJiIhUMCyJiIhUMCyJiIhUMCyJiIhUMCyJiIhUMCyJApk0vAAADQVJREFUiIhUMCyJiIhUMCyJiIhUMCyJiIhUMCyJiIhUMCyJiIhUMCyJiIhUMCyJiIhUMCzJbRmNRiiK4lXtlpaWIiYmBoqiICYmBrt27bridRYUFCAhIQGKokBRFCQkJKCoqAgGg0HK82cvb3x9yXsxLMlt5eXleVW7RqMRRUVFSElJQXl5OQYNGoTBgwcjNze30etMSEjA22+/jUmTJkEIASEE5syZg9LSUrRt29aB1Tuet72+5N0YluSWjEYj0tLSvKrdvLw8aLVaAEDz5s0RHh4OAAgNDW3U+syfIFNSUtC1a1fL9DZt2kCr1SI/P//Ki3YSb3x9ybsxLH2I0WhEVlaW5XCdrZ2GrWUMBoNlvsFgQFZWlmUHn5ubC0VREBoaitLS0ga1Z95xWR8+NLeVlJRk+cRlnm9dQ3JysqVd86FMe2tzdLv2MgdlTTqdrtr9hIQEJCQk1LuugoICrFixAosWLapzmb59+9aaxtfXea8veTlBHicyMlJERkY2+HFarVbEx8db7ut0umr3zcukpqYKIYQoKysTWq1WaLVaUV5ebpkPQAAQ+fn5Qggh9Hq9ACB0Ol2D2tPpdAKAKCsrs7kOczvWzDVlZmYKIYTYuXOnACAKCwvtrs3R7TZWeXm5ACBycnKqTY+Pj6/1utQUHx9v2YaG4Ovr3NcXgEhPT2/QY8gjxDIsPVBjwjIzM7PWzjU/P19otVrLffMOouYyACw7ESFs72xqTrOnvfj4+Hp3YrbaMa+3ZtvmnbQ9tTmj3cbYuXNntaBqCFs12tMeX1/nvr4MS6/FsPREjQlL87vy+pjfkVszf/qx3gnas8Oypz0zvV4vkpKS7NqpWX+6qHmztzZntNsYWq3W8gmpoRrTNl/fxrdrL4al12JYeqLGhKU9//HrWsaenY09y9iSmpoqtFqtKCkpaVQ79myDrWmObrehMjMzLYdDG8McfA35VMrXt/Ht2oth6bUYlp7oSj5Z1ncOxrxMzfNggPo5n7o+edTXnvnQl16vt7mO+topKSmxuU57anNGuw1RWFh4RYdvhRAiJydH9fmtia9v49u1F8PSa8VyNKyPMI/EfPXVV2E0GgH87wvyZpGRkQCAb7/91jLNvOz48eMd3l5ERAQAoEOHDnavNzU1FQCwceNGy3rNoxjtJatd82N27NiBxMREy7SioqJqz4s9tFottFotXn311TqXKS0trVYfX1/ntkteTnZcU8M15pOlebQfrM7F6HS6au+ky8vLLaMjzZ8+MjMzq33qKCsrszzefAjQfN4LVp9a7GnPPF+v11c7XGZeh/UnoaSkpFrtW9/0er3dtTm63St5Dcw36xGx9oyGtV5fzedViMvn66xfR/NzwdfXea+vEPxk6cV4GNYTNfarI2VlZZavHMTHx9s85FRWViZSU1MtO4vMzMxq58Vq7kzqmmZPe4WFhZZ55mV1Op1lB1Vzvpler7es13p5e2tzdLv2Mp9ntHWzfm7sDUshLodFTk5OtXWbvx5iqz6+vs57fc31MCy9UqwihBAgjxIVFQUASE9Pl1wJEVlTFAXp6emWQ97kNWbznCUREZEKhiUREZGKANkFEHk6e3/uiWc8iDwXw5LoCjEEibwfD8MSERGpYFgSERGpYFgSERGpYFgSERGpYFgSERGpYFgSERGpYFgSERGpYFgSERGpYFgSERGpYFgSERGpYFgSERGpYFgSERGpYFgSERGp4K+OeKjs7GyMGTNGdhlERD6BYemBbrnlFphMJoSFhckuhYhq6NKli+wSyAkUwR/jI5ImIyMDUVFR/E1MIvc2m+csiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVDAsiYiIVATILoDIl+zcuRMnTpyw3N+/fz8AIDU1tdpyw4cPR4cOHVxaGxHVTRFCCNlFEPkKRVEAABqNBgAghIAQAn5+/zvIYzKZsGDBAqxatUpKjURUy2wehiVyoejoaGg0GphMJphMJlRUVKCystJy32QyAQBCQkIkV0pE1hiWRC4UERFhCcS6XHfddRgyZIiLKiIiezAsiVwoJCQErVq1qnO+RqNBeHg4AgI4nIDInTAsiVzI398fEydORGBgoM35JpMJkZGRLq6KiNQwLIlcLDIyEpcuXbI5r127dujfv7+LKyIiNQxLIhfr06cP2rdvX2u6RvP/7d0/aBN9HMfxz2GL4GKmFBEaECkoQpzcFcHpslVMxNZBJWIL+thJUkTQrQG3SlNwkDSBupiAk3WQB9RBiKMiyOmUm5LZyu8Z9I7E2v6Smid3se8XhDb37/e9o/TTu/teOq6ZmZmwYxZAfBCWwJA5jqPZ2dnw8ZHAt2/fdOHChYiqArATwhKIQC6X29IVe/ToUaXT6YgqArATwhKIwPHjx3Xs2LHw/fj4uC5fvhxdQQB2RFgCEZmZmQkvxW5ubiqbzUZcEYDtEJZARLLZrDY3NyVJJ0+e1JEjRyKuCMB2CEsgIqlUKrxHOTs7G3E1AHbCB6ljaAqFgh48eBB1GRgRb9++1alTp6IuA5CkOT5TC0Pz+fNnjY+Pq1wuR11KbHz//l2+7+vQoUNRlxIr58+f16dPnwhLxAZhiaGanp7W9PR01GUAQF+4ZwkAgAVhCQCABWEJAIAFYQkAgAVhCQCABWEJAIAFYQkAgAVhCQCABWEJAIAFYQkAgAVhCQCABWEJAIAFYQkAgAVhCQCABWGJkeP7vqrVqjKZTNSlANgjCEuMnLt37yqbzaper0ddykCUSiU5jtPXOo7jbPsqFouq1+tqt9v/U8XA3kNYYuQsLy9HXcLAvH//XteuXet7PWOMms1m+L7VaskYI2OMzp49q1KppEuXLsn3/UGWC+xZhCUQkXa7radPn+56/WQyGX5/8ODB8Pt0Oq3V1VVJ0pUrVzjDBAaAsETstdttVatVOY6jTCajjx8//nY53/dVLBbD5V6+fBlO77zHWa/Xw2W+fPnStY1g/VKpJN/3t1we3W6M3VhdXdX8/Pxv5y0uLmpxcXHX204mk7p586bq9bpevXrVNW/UjhMQCwYYklwuZ3K5XN/rua5r8vm8abVaxhhjKpWKkWQ6f3ybzaZxXddUKhVjjDEbGxtGkmk0GsZ13XD5169fG2OM8TzPSDL5fD7cxtLSkvE8zxhjTKvVMoVCoecx+rWxsRHW8uu+GGNMoVAwhULBup3frRtotVpb9nFUjpMkUy6Xe14e+J/dICwxNLsJy1qtZiSZDx8+hNOCEOj8BR0EaCdJYeD8LlR+nSbJNJvN8H2z2exrjF41m02zsrKybR39sK07qseJsETM3OAyLGLt+fPnkqSpqalwWuf9ucDa2pqk7i5RSbp//37PY+XzeU1MTKharardbiuZTMoYM9AxJOnZs2e6evVqX+sMyigdJyBOCEvE2qNHj3paLniMxPzsCO189erWrVtyXVfZbFaJRELFYnHgY9TrdZ07d67n5f9E0NhTKBS6xpfif5yAuCEs8VfZrvmnF1NTU6rVamo0Gsrn81pYWNgSBH86RiaTUSqV2nLWJanvZy1t3r17J0k6ffr0lnlxP05A3BCWiLWVlRVJP55H7GW5J0+ehGdUQUdmrxzHUbvdVjqd1vLyshqNhhYWFgY6xk5nW4M88/J9Xw8fPpTrujpz5kw4fVSOExA7w7k3CuyuwSfoxnRdN+zADLor1dGlGTSZ/PryPK9rXtBR29kkFDSr6GcTSjCO53lmaWkprGWnMf6EtLtu2M59CPbLGBN2trqu29WIY9uHOB0n0eCDeKHBB/E2OTkpz/N0+PBhpVIpXb9+XSdOnJDruqpUKrp3756kH88Vep4X3p/L5/PyPE+Tk5OamJgIt5dIJLq+SuqaPz8/r/X1dTmOo/X1dd2+fTuct9MYw+Y4Ttc+JBKJ8LLuixcvdOfOHdVqta4PLpD23nECBsUxhrvuGI6LFy9KksrlcsSVIO4cx1G5XFYul4u6FECS5jizBADAgrAEAMBiLOoCgL9Br499cNcDGE2EJTAAhCDwd+MyLAAAFoQlAAAWhCUAABaEJQAAFoQlAAAWhCUAABaEJQAAFoQlAAAWhCUAABaEJQAAFoQlAAAWhCUAABaEJQAAFvzXEQzN/v379fjxY62trUVdCkbAgQMHoi4BCDmG/y2EIfn69avevHkTdRkYAfv27VMmk9HYGH/PIxbmCEsAAHY2xz1LAAAsCEsAACwISwAALMYk/RN1EQAAxNi//wHAYYLyNim0lwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pydot\n",
    "tf.keras.utils.plot_model(m8, \"m8.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbf3b3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1000. 1000.]\n",
      " [1000. 1000.]]\n"
     ]
    }
   ],
   "source": [
    "hold = m8.layers[6].get_weights()[0]\n",
    "for h in range(len(hold)):\n",
    "    for n in range(len(hold[h])):\n",
    "        hold[h][n] = 1000\n",
    "print(hold)\n",
    "m8.layers[6].set_weights([hold, m8.layers[6].get_weights()[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a5674104",
   "metadata": {},
   "outputs": [],
   "source": [
    "m8.layers[6].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8fd61215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 4)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           80          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "trump1 (Dense)                  (None, 1)            17          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "trump2 (Dense)                  (None, 1)            17          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "trump3 (Dense)                  (None, 1)            2           trump1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "trump4 (Dense)                  (None, 1)            2           trump2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           272         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 18)           0           trump3[0][0]                     \n",
      "                                                                 trump4[0][0]                     \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 2)            38          concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 428\n",
      "Trainable params: 424\n",
      "Non-trainable params: 4\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m8.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "523b7c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.2981e-04 - accuracy: 1.0000\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.2317e-04 - accuracy: 0.9999\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3930e-04 - accuracy: 0.9999\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.1997e-04 - accuracy: 1.0000\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.9038e-04 - accuracy: 0.9999\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.3415e-04 - accuracy: 1.0000\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.1668e-04 - accuracy: 1.0000\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 9.2358e-04 - accuracy: 0.9999\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.5878e-04 - accuracy: 0.9999\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.7554e-04 - accuracy: 0.9999\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2245e-04 - accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.3765e-04 - accuracy: 0.9999\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.6879e-04 - accuracy: 0.9999\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 9.1923e-04 - accuracy: 0.9999\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.0449e-04 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.0508e-04 - accuracy: 0.9999\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.1630e-04 - accuracy: 0.9999\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.3369e-04 - accuracy: 0.9999\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.5130e-04 - accuracy: 0.9999\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.6704e-04 - accuracy: 0.9999\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.8738e-04 - accuracy: 0.9999\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.1069e-04 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.1382e-04 - accuracy: 0.9999\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.3732e-04 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.4000e-04 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.9986e-04 - accuracy: 0.9999\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.2854e-04 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.8589e-04 - accuracy: 0.9999\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.1107e-04 - accuracy: 0.9999\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.0698e-04 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.2205e-04 - accuracy: 0.9999\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.4628e-04 - accuracy: 0.9999\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.0070e-04 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.6309e-04 - accuracy: 0.9999\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.0560e-04 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9999\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.1147e-04 - accuracy: 0.9999\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.5376e-04 - accuracy: 0.9999\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.7597e-04 - accuracy: 0.9999\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.0479e-04 - accuracy: 0.9999\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 6.7107e-05 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.3503e-04 - accuracy: 0.9999\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.0803e-04 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.2571e-04 - accuracy: 0.9999\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.4706e-04 - accuracy: 0.9999\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.7515e-04 - accuracy: 0.9999\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.2431e-04 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.9541e-04 - accuracy: 0.9999\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.7896e-04 - accuracy: 0.9999\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 9.8379e-05 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 5.3978e-04 - accuracy: 0.9999\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.0936e-04 - accuracy: 0.9999\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 5.5560e-04 - accuracy: 0.9999\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.4786e-04 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.2906e-04 - accuracy: 0.9999\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.1910e-04 - accuracy: 0.9999\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 9.8279e-05 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.1709e-04 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 8.8158e-05 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.1549e-04 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 7.6093e-05 - accuracy: 0.9999\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.8506e-04 - accuracy: 0.9999\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.0196e-04 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.2702e-04 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.7776e-04 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.6530e-04 - accuracy: 0.9999\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 5.2173e-04 - accuracy: 0.9999\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 5.1264e-05 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 9.1769e-04 - accuracy: 0.9998\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.4584e-04 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.9748e-04 - accuracy: 0.9999\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.1552e-04 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.8963e-05 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.6860e-04 - accuracy: 0.9999\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 5.3545e-04 - accuracy: 0.9999\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 2ms/step - loss: 2.2593e-04 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.4512e-04 - accuracy: 0.9999\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.2982e-04 - accuracy: 0.9999\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3483e-04 - accuracy: 0.9999\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 8.9070e-05 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 1.3231e-04 - accuracy: 0.9999\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 4.8258e-05 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 8.0713e-05 - accuracy: 0.9999\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 6.9994e-04 - accuracy: 0.9998\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.3964e-04 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 8.6043e-05 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3762e-04 - accuracy: 0.9999\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.3840e-04 - accuracy: 0.9999\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 8.7677e-05 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.4565e-04 - accuracy: 0.9999\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 3.5058e-04 - accuracy: 0.9999\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 2.4672e-04 - accuracy: 0.9999\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 7.8623e-05 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.6572e-04 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 9.4706e-05 - accuracy: 0.9999\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 7.3415e-05 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23701f5c7f0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m8.fit(inputs,labels,epochs = 100, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8da2451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hold=m8.layers[4].get_weights()\n",
    "hold[0][0] = 1000\n",
    "m8.layers[4].set_weights(hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a6d3ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hold=m8.layers[5].get_weights()\n",
    "hold[0][0] = 1000\n",
    "m8.layers[5].set_weights(hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a877d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "m8.layers[4].trainable = False\n",
    "m8.layers[5].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3cdec0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m8.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3438d26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.01445679,  1.0169357 ,  0.001355  , -0.17394519, -0.11578245,\n",
       "          0.16278501, -0.08392554,  0.53784215,  0.00311578,  0.19300498,\n",
       "         -0.01785773,  0.7544893 , -0.00638653,  0.01407889, -0.0020919 ,\n",
       "         -0.0014707 ],\n",
       "        [ 0.11002993, -0.00839826,  0.55979097,  1.1150556 , -0.46857595,\n",
       "         -0.55178434, -0.44756293, -0.38170895, -0.7689091 ,  0.12998173,\n",
       "         -0.05366521,  0.77087885, -0.3003912 , -0.51304233,  1.1048595 ,\n",
       "          0.0034122 ],\n",
       "        [-0.0169918 , -0.03668987, -0.00280021, -0.2258041 ,  0.9793205 ,\n",
       "          0.04587241, -0.14183442,  0.03088209, -0.09728538, -0.5996637 ,\n",
       "          0.940471  ,  0.21063007, -0.02527909, -0.01401419, -0.00141205,\n",
       "         -0.65591013],\n",
       "        [ 0.15075941, -0.00257705, -0.00148449,  0.44824728, -0.31248343,\n",
       "         -0.26485637,  0.4293996 ,  0.00524454,  0.08863128, -0.30988744,\n",
       "         -0.00524183, -0.10924517, -0.10221187,  0.13925588, -0.12294307,\n",
       "         -0.37227076]], dtype=float32),\n",
       " array([-2.1186154 , -1.817306  , -1.1276733 ,  0.19136071,  0.39934748,\n",
       "         0.64710647, -0.9830168 , -1.0396439 ,  0.46630841,  0.35834083,\n",
       "        -1.801721  ,  0.34391624,  2.681391  , -2.268646  ,  0.3912448 ,\n",
       "         1.1329651 ], dtype=float32),\n",
       " array([[ 0.15824002],\n",
       "        [-0.29207438],\n",
       "        [ 0.4154144 ],\n",
       "        [-0.7299347 ],\n",
       "        [ 0.5628903 ],\n",
       "        [ 0.6878694 ],\n",
       "        [-0.23549767],\n",
       "        [-0.08800764],\n",
       "        [ 0.13510096],\n",
       "        [-0.5310616 ],\n",
       "        [ 0.26139426],\n",
       "        [ 0.4355095 ],\n",
       "        [-0.05528263],\n",
       "        [ 0.17985639],\n",
       "        [ 0.55182606],\n",
       "        [ 0.78067255]], dtype=float32),\n",
       " array([0.14817376], dtype=float32),\n",
       " array([[ 0.41643745],\n",
       "        [-0.1587712 ],\n",
       "        [ 1.4787464 ],\n",
       "        [ 0.15730146],\n",
       "        [ 0.04324703],\n",
       "        [ 0.01129055],\n",
       "        [-0.3333563 ],\n",
       "        [ 0.33479726],\n",
       "        [ 0.06273699],\n",
       "        [ 0.7100366 ],\n",
       "        [ 2.9141407 ],\n",
       "        [ 0.26870573],\n",
       "        [ 0.48054817],\n",
       "        [ 0.11105543],\n",
       "        [ 1.3163558 ],\n",
       "        [-0.542654  ]], dtype=float32),\n",
       " array([-1.1215398], dtype=float32),\n",
       " array([[999.9908]], dtype=float32),\n",
       " array([-0.02380352], dtype=float32),\n",
       " array([[999.4647]], dtype=float32),\n",
       " array([-1.5117905], dtype=float32),\n",
       " array([[-0.29589525, -0.2947292 ,  0.24937871, -0.00755181, -0.05963995,\n",
       "         -0.29377192, -0.5307482 , -0.54602003,  0.17510018,  0.52769405,\n",
       "          0.20433283,  0.1215184 , -0.4700832 ,  0.37796056,  0.31807223,\n",
       "          0.28731495],\n",
       "        [ 0.08158208,  0.6222724 ,  1.6330827 , -0.11296583,  1.6517521 ,\n",
       "         -1.5695498 ,  0.45692202,  0.20219402,  0.7232704 ,  0.02176642,\n",
       "          1.2957554 ,  3.0721145 , -0.5356538 ,  0.41383466,  1.1768104 ,\n",
       "         -0.05137058],\n",
       "        [ 0.10392028,  0.5437667 ,  1.7348895 , -0.45708117,  2.149416  ,\n",
       "          1.9384317 , -0.4348828 , -0.83802474,  1.7989423 , -1.2163562 ,\n",
       "          1.8690886 ,  1.6970739 ,  1.9535656 ,  0.21582276,  1.953962  ,\n",
       "         -0.04768815],\n",
       "        [-0.29632646, -0.08199541,  0.09838409, -0.42021492,  0.01768401,\n",
       "          0.3448373 ,  0.09381231,  0.29444218,  0.00649323,  0.8476356 ,\n",
       "          0.15122221, -0.10196199,  0.15124817, -0.241724  ,  0.16991842,\n",
       "         -0.476385  ],\n",
       "        [-0.1100601 , -0.37333012,  0.01510982, -0.02396193,  1.2154928 ,\n",
       "         -0.24837378, -0.8654313 ,  0.9783741 ,  0.35190362,  1.5960569 ,\n",
       "          0.02687213,  0.2218935 ,  0.3381885 ,  0.7647656 ,  0.6013642 ,\n",
       "          0.05336642],\n",
       "        [-0.46033815,  2.3706079 ,  1.015252  , -0.45081148,  1.0596994 ,\n",
       "          0.661659  ,  0.54849577,  0.96472204,  0.9733278 , -0.23423082,\n",
       "          0.19872636,  0.46980166,  0.8167011 ,  0.6589773 ,  1.2175841 ,\n",
       "          0.22485055],\n",
       "        [-0.35190225,  0.12768014, -0.02792934, -0.20384149,  0.27493402,\n",
       "         -0.27337614, -0.33310914, -0.6246569 ,  0.24989034,  0.28335926,\n",
       "         -0.1632662 ,  0.2671207 , -0.23419434, -0.4356264 , -0.25389072,\n",
       "          0.01498719],\n",
       "        [-0.05232136,  0.06313653,  0.3718205 ,  0.16962388,  0.61358386,\n",
       "         -0.0412302 , -0.04615137,  0.28011054,  0.3504179 ,  1.346548  ,\n",
       "          1.6681591 , -0.04538992,  0.23760688,  0.2352328 ,  1.2121422 ,\n",
       "         -0.4007637 ],\n",
       "        [-0.36716613, -0.35369754, -0.5177466 ,  0.05506647, -0.7056388 ,\n",
       "         -0.14704171, -0.09773535,  0.4974022 , -0.7678409 ,  0.2965335 ,\n",
       "         -0.2592651 , -0.23868869, -0.05962523, -0.11157481, -0.05854823,\n",
       "          0.01438924],\n",
       "        [ 0.36232698,  0.30091295,  1.2680521 , -0.04036814,  0.49502197,\n",
       "         -0.20161109,  0.6872226 , -0.15318523,  0.8037844 , -0.09333823,\n",
       "          0.6592225 ,  0.82621783,  0.23346196,  0.2814595 ,  0.4601835 ,\n",
       "         -0.11075016],\n",
       "        [-0.04308924,  0.0579297 ,  3.3330145 , -0.26117453,  2.5787945 ,\n",
       "         -0.29778272,  0.6254857 , -1.1489084 ,  3.7213361 , -2.6159434 ,\n",
       "         -1.9404975 ,  3.0421295 , -2.0608506 ,  1.2381797 , -1.7986745 ,\n",
       "         -0.20140441],\n",
       "        [-0.3670621 ,  0.19296965,  0.52278394, -0.02763508,  0.14018252,\n",
       "          0.6779169 ,  0.9085728 ,  0.48845178, -0.21437164,  0.06537744,\n",
       "          0.15683231,  0.4128494 ,  0.42214087,  0.10684399, -0.0056878 ,\n",
       "         -0.37064454],\n",
       "        [-0.15440395,  0.92295235,  0.7471376 , -0.12931585, -0.2541465 ,\n",
       "          1.7523364 ,  1.2258744 ,  1.0164464 ,  0.9242996 , -0.15878202,\n",
       "          0.09315614, -1.3395573 ,  1.4329625 ,  0.32150987,  0.147566  ,\n",
       "          0.18948425],\n",
       "        [-0.30252737, -0.37881947,  0.29397377, -0.05225484,  0.3139562 ,\n",
       "         -0.15313871, -0.18406744, -0.24959457,  0.01112312,  0.45288107,\n",
       "         -0.11287207,  0.28693545, -0.322092  ,  0.20631912, -0.08340101,\n",
       "         -0.3591976 ],\n",
       "        [ 0.19817647,  1.7600825 ,  1.4535279 ,  0.1030358 ,  0.8707381 ,\n",
       "          2.2355156 ,  2.40501   ,  2.008438  ,  2.2981355 , -1.6188174 ,\n",
       "          2.5928292 ,  0.8072414 ,  2.0329213 ,  1.011891  ,  1.245088  ,\n",
       "         -0.02918893],\n",
       "        [-0.22412318,  0.9387468 ,  2.8099802 , -0.3758077 ,  1.024296  ,\n",
       "          0.588538  ,  1.168053  ,  0.6408243 ,  2.6809254 , -0.7177525 ,\n",
       "          1.0642321 ,  0.05808355,  0.584399  , -0.24211037,  1.139773  ,\n",
       "         -0.4823764 ]], dtype=float32),\n",
       " array([-0.0518941 ,  0.8557935 , -1.35235   , -0.0418533 , -1.8447055 ,\n",
       "         0.4826709 ,  2.1905336 ,  1.220928  , -1.6704483 ,  0.26780233,\n",
       "        -1.1001208 , -1.3003302 ,  0.52940756, -1.2616473 , -0.9655494 ,\n",
       "        -0.12692456], dtype=float32),\n",
       " array([[-0.22408096, -0.5694773 ],\n",
       "        [-0.20914067, -0.25320727],\n",
       "        [-0.34906378,  0.41567272],\n",
       "        [ 2.0714984 , -1.6496605 ],\n",
       "        [ 1.4044126 , -1.1057996 ],\n",
       "        [-0.3813959 , -0.24417129],\n",
       "        [ 1.5153399 , -1.462946  ],\n",
       "        [ 2.0599802 , -2.969194  ],\n",
       "        [ 9.13317   , -8.249173  ],\n",
       "        [ 2.6359172 , -2.4665434 ],\n",
       "        [ 0.8532683 , -0.80503094],\n",
       "        [-0.88707405,  0.49350318],\n",
       "        [ 2.9977038 , -3.2510514 ],\n",
       "        [ 2.3975456 , -2.2056816 ],\n",
       "        [ 1.3153585 , -1.832469  ],\n",
       "        [ 0.6193016 , -0.8126544 ],\n",
       "        [ 1.9912182 , -2.4641933 ],\n",
       "        [ 0.04732463,  0.18659507]], dtype=float32),\n",
       " array([-0.27879488,  0.2787601 ], dtype=float32)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m8.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "22307878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Public\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - cudatoolkit=10.2\n",
      "    - pytorch\n",
      "    - torchaudio\n",
      "\n",
      "    - torchvision\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-4.11.0               |   py38haa95532_0        14.4 MB\n",
      "    cudatoolkit-10.2.89        |       h74a9793_1       317.2 MB\n",
      "    libuv-1.40.0               |       he774522_0         255 KB\n",
      "    pytorch-1.10.0             |py3.8_cuda10.2_cudnn7_0       950.8 MB  pytorch\n",
      "    pytorch-mutex-1.0          |             cuda           3 KB  pytorch\n",
      "    torchaudio-0.10.0          |       py38_cu102         2.1 MB  pytorch\n",
      "    torchvision-0.11.1         |       py38_cu102         8.5 MB  pytorch\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        1.26 GB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cudatoolkit        pkgs/main/win-64::cudatoolkit-10.2.89-h74a9793_1\n",
      "  libuv              pkgs/main/win-64::libuv-1.40.0-he774522_0\n",
      "  pytorch            pytorch/win-64::pytorch-1.10.0-py3.8_cuda10.2_cudnn7_0\n",
      "  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cuda\n",
      "  torchaudio         pytorch/win-64::torchaudio-0.10.0-py38_cu102\n",
      "  torchvision        pytorch/win-64::torchvision-0.11.1-py38_cu102\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda                               4.10.3-py38haa95532_0 --> 4.11.0-py38haa95532_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "torchaudio-0.10.0    | 2.1 MB    |            |   0% \n",
      "torchaudio-0.10.0    | 2.1 MB    |            |   1% \n",
      "torchaudio-0.10.0    | 2.1 MB    | ######8    |  69% \n",
      "torchaudio-0.10.0    | 2.1 MB    | ########## | 100% \n",
      "\n",
      "libuv-1.40.0         | 255 KB    |            |   0% \n",
      "libuv-1.40.0         | 255 KB    | ########## | 100% \n",
      "libuv-1.40.0         | 255 KB    | ########## | 100% \n",
      "\n",
      "torchvision-0.11.1   | 8.5 MB    |            |   0% \n",
      "torchvision-0.11.1   | 8.5 MB    |            |   0% \n",
      "torchvision-0.11.1   | 8.5 MB    | ##2        |  22% \n",
      "torchvision-0.11.1   | 8.5 MB    | ####4      |  44% \n",
      "torchvision-0.11.1   | 8.5 MB    | ######5    |  66% \n",
      "torchvision-0.11.1   | 8.5 MB    | ########8  |  89% \n",
      "torchvision-0.11.1   | 8.5 MB    | ########## | 100% \n",
      "\n",
      "pytorch-mutex-1.0    | 3 KB      |            |   0% \n",
      "pytorch-mutex-1.0    | 3 KB      | ########## | 100% \n",
      "pytorch-mutex-1.0    | 3 KB      | ########## | 100% \n",
      "\n",
      "conda-4.11.0         | 14.4 MB   |            |   0% \n",
      "conda-4.11.0         | 14.4 MB   | 8          |   8% \n",
      "conda-4.11.0         | 14.4 MB   | #9         |  19% \n",
      "conda-4.11.0         | 14.4 MB   | ###1       |  31% \n",
      "conda-4.11.0         | 14.4 MB   | ####2      |  43% \n",
      "conda-4.11.0         | 14.4 MB   | #####6     |  56% \n",
      "conda-4.11.0         | 14.4 MB   | ######9    |  70% \n",
      "conda-4.11.0         | 14.4 MB   | ########3  |  84% \n",
      "conda-4.11.0         | 14.4 MB   | #########7 |  97% \n",
      "conda-4.11.0         | 14.4 MB   | ########## | 100% \n",
      "\n",
      "pytorch-1.10.0       | 950.8 MB  |            |   0% \n",
      "pytorch-1.10.0       | 950.8 MB  |            |   0% \n",
      "pytorch-1.10.0       | 950.8 MB  |            |   0% \n",
      "pytorch-1.10.0       | 950.8 MB  |            |   0% \n",
      "pytorch-1.10.0       | 950.8 MB  |            |   1% \n",
      "pytorch-1.10.0       | 950.8 MB  |            |   1% \n",
      "pytorch-1.10.0       | 950.8 MB  | 1          |   1% \n",
      "pytorch-1.10.0       | 950.8 MB  | 1          |   1% \n",
      "pytorch-1.10.0       | 950.8 MB  | 1          |   2% \n",
      "pytorch-1.10.0       | 950.8 MB  | 1          |   2% \n",
      "pytorch-1.10.0       | 950.8 MB  | 1          |   2% \n",
      "pytorch-1.10.0       | 950.8 MB  | 2          |   2% \n",
      "pytorch-1.10.0       | 950.8 MB  | 2          |   2% \n",
      "pytorch-1.10.0       | 950.8 MB  | 2          |   3% \n",
      "pytorch-1.10.0       | 950.8 MB  | 2          |   3% \n",
      "pytorch-1.10.0       | 950.8 MB  | 3          |   3% \n",
      "pytorch-1.10.0       | 950.8 MB  | 3          |   3% \n",
      "pytorch-1.10.0       | 950.8 MB  | 3          |   3% \n",
      "pytorch-1.10.0       | 950.8 MB  | 3          |   4% \n",
      "pytorch-1.10.0       | 950.8 MB  | 3          |   4% \n",
      "pytorch-1.10.0       | 950.8 MB  | 4          |   4% \n",
      "pytorch-1.10.0       | 950.8 MB  | 4          |   4% \n",
      "pytorch-1.10.0       | 950.8 MB  | 4          |   5% \n",
      "pytorch-1.10.0       | 950.8 MB  | 4          |   5% \n",
      "pytorch-1.10.0       | 950.8 MB  | 5          |   5% \n",
      "pytorch-1.10.0       | 950.8 MB  | 5          |   5% \n",
      "pytorch-1.10.0       | 950.8 MB  | 5          |   5% \n",
      "pytorch-1.10.0       | 950.8 MB  | 5          |   6% \n",
      "pytorch-1.10.0       | 950.8 MB  | 5          |   6% \n",
      "pytorch-1.10.0       | 950.8 MB  | 6          |   6% \n",
      "pytorch-1.10.0       | 950.8 MB  | 6          |   6% \n",
      "pytorch-1.10.0       | 950.8 MB  | 6          |   7% \n",
      "pytorch-1.10.0       | 950.8 MB  | 6          |   7% \n",
      "pytorch-1.10.0       | 950.8 MB  | 7          |   7% \n",
      "pytorch-1.10.0       | 950.8 MB  | 7          |   7% \n",
      "pytorch-1.10.0       | 950.8 MB  | 7          |   7% \n",
      "pytorch-1.10.0       | 950.8 MB  | 7          |   8% \n",
      "pytorch-1.10.0       | 950.8 MB  | 7          |   8% \n",
      "pytorch-1.10.0       | 950.8 MB  | 8          |   8% \n",
      "pytorch-1.10.0       | 950.8 MB  | 8          |   8% \n",
      "pytorch-1.10.0       | 950.8 MB  | 8          |   9% \n",
      "pytorch-1.10.0       | 950.8 MB  | 8          |   9% \n",
      "pytorch-1.10.0       | 950.8 MB  | 8          |   9% \n",
      "pytorch-1.10.0       | 950.8 MB  | 9          |   9% \n",
      "pytorch-1.10.0       | 950.8 MB  | 9          |   9% \n",
      "pytorch-1.10.0       | 950.8 MB  | 9          |  10% \n",
      "pytorch-1.10.0       | 950.8 MB  | 9          |  10% \n",
      "pytorch-1.10.0       | 950.8 MB  | 9          |  10% \n",
      "pytorch-1.10.0       | 950.8 MB  | #          |  10% \n",
      "pytorch-1.10.0       | 950.8 MB  | #          |  10% \n",
      "pytorch-1.10.0       | 950.8 MB  | #          |  11% \n",
      "pytorch-1.10.0       | 950.8 MB  | #          |  11% \n",
      "pytorch-1.10.0       | 950.8 MB  | #1         |  11% \n",
      "pytorch-1.10.0       | 950.8 MB  | #1         |  11% \n",
      "pytorch-1.10.0       | 950.8 MB  | #1         |  11% \n",
      "pytorch-1.10.0       | 950.8 MB  | #1         |  12% \n",
      "pytorch-1.10.0       | 950.8 MB  | #1         |  12% \n",
      "pytorch-1.10.0       | 950.8 MB  | #2         |  12% \n",
      "pytorch-1.10.0       | 950.8 MB  | #2         |  12% \n",
      "pytorch-1.10.0       | 950.8 MB  | #2         |  12% \n",
      "pytorch-1.10.0       | 950.8 MB  | #2         |  13% \n",
      "pytorch-1.10.0       | 950.8 MB  | #2         |  13% \n",
      "pytorch-1.10.0       | 950.8 MB  | #3         |  13% \n",
      "pytorch-1.10.0       | 950.8 MB  | #3         |  13% \n",
      "pytorch-1.10.0       | 950.8 MB  | #3         |  13% \n",
      "pytorch-1.10.0       | 950.8 MB  | #3         |  14% \n",
      "pytorch-1.10.0       | 950.8 MB  | #3         |  14% \n",
      "pytorch-1.10.0       | 950.8 MB  | #3         |  14% \n",
      "pytorch-1.10.0       | 950.8 MB  | #4         |  14% \n",
      "pytorch-1.10.0       | 950.8 MB  | #4         |  14% \n",
      "pytorch-1.10.0       | 950.8 MB  | #4         |  15% \n",
      "pytorch-1.10.0       | 950.8 MB  | #4         |  15% \n",
      "pytorch-1.10.0       | 950.8 MB  | #4         |  15% \n",
      "pytorch-1.10.0       | 950.8 MB  | #4         |  15% \n",
      "pytorch-1.10.0       | 950.8 MB  | #5         |  15% \n",
      "pytorch-1.10.0       | 950.8 MB  | #5         |  15% \n",
      "pytorch-1.10.0       | 950.8 MB  | #5         |  15% \n",
      "pytorch-1.10.0       | 950.8 MB  | #5         |  16% \n",
      "pytorch-1.10.0       | 950.8 MB  | #5         |  16% \n",
      "pytorch-1.10.0       | 950.8 MB  | #5         |  16% \n",
      "pytorch-1.10.0       | 950.8 MB  | #6         |  16% \n",
      "pytorch-1.10.0       | 950.8 MB  | #6         |  16% \n",
      "pytorch-1.10.0       | 950.8 MB  | #6         |  16% \n",
      "pytorch-1.10.0       | 950.8 MB  | #6         |  17% \n",
      "pytorch-1.10.0       | 950.8 MB  | #6         |  17% \n",
      "pytorch-1.10.0       | 950.8 MB  | #6         |  17% \n",
      "pytorch-1.10.0       | 950.8 MB  | #6         |  17% \n",
      "pytorch-1.10.0       | 950.8 MB  | #7         |  17% \n",
      "pytorch-1.10.0       | 950.8 MB  | #7         |  17% \n",
      "pytorch-1.10.0       | 950.8 MB  | #7         |  17% \n",
      "pytorch-1.10.0       | 950.8 MB  | #7         |  18% \n",
      "pytorch-1.10.0       | 950.8 MB  | #7         |  18% \n",
      "pytorch-1.10.0       | 950.8 MB  | #8         |  18% \n",
      "pytorch-1.10.0       | 950.8 MB  | #8         |  18% \n",
      "pytorch-1.10.0       | 950.8 MB  | #8         |  18% \n",
      "pytorch-1.10.0       | 950.8 MB  | #8         |  19% \n",
      "pytorch-1.10.0       | 950.8 MB  | #8         |  19% \n",
      "pytorch-1.10.0       | 950.8 MB  | #8         |  19% \n",
      "pytorch-1.10.0       | 950.8 MB  | #9         |  19% \n",
      "pytorch-1.10.0       | 950.8 MB  | #9         |  19% \n",
      "pytorch-1.10.0       | 950.8 MB  | #9         |  19% \n",
      "pytorch-1.10.0       | 950.8 MB  | #9         |  20% \n",
      "pytorch-1.10.0       | 950.8 MB  | #9         |  20% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##         |  20% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##         |  20% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##         |  20% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##         |  21% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##         |  21% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##1        |  21% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##1        |  21% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##1        |  21% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##1        |  22% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##1        |  22% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##2        |  22% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##2        |  22% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##2        |  23% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##2        |  23% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##2        |  23% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##3        |  23% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##3        |  23% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##3        |  23% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##3        |  24% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##3        |  24% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##4        |  24% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##4        |  24% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##4        |  24% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##4        |  25% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##4        |  25% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##4        |  25% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##5        |  25% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##5        |  25% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##5        |  25% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##5        |  26% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##5        |  26% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##6        |  26% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##6        |  26% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##6        |  26% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##6        |  27% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##6        |  27% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##6        |  27% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##7        |  27% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##7        |  27% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##7        |  27% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##7        |  28% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##7        |  28% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##7        |  28% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##8        |  28% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##8        |  28% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##8        |  28% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##8        |  29% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##8        |  29% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##8        |  29% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##9        |  29% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##9        |  29% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##9        |  30% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##9        |  30% \n",
      "pytorch-1.10.0       | 950.8 MB  | ##9        |  30% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###        |  30% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###        |  30% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###        |  30% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###        |  31% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###        |  31% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###        |  31% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###1       |  31% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###1       |  31% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###1       |  32% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###1       |  32% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###1       |  32% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###2       |  32% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###2       |  32% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###2       |  33% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###2       |  33% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###2       |  33% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###3       |  33% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###3       |  33% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###3       |  33% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###3       |  34% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###3       |  34% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###3       |  34% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###4       |  34% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###4       |  34% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###4       |  34% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###4       |  35% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###4       |  35% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###5       |  35% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###5       |  35% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###5       |  35% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###5       |  36% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###5       |  36% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###5       |  36% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###6       |  36% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###6       |  36% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###6       |  37% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###6       |  37% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###6       |  37% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###7       |  37% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###7       |  37% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###7       |  38% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###7       |  38% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###7       |  38% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###8       |  38% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###8       |  38% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###8       |  38% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###8       |  38% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###8       |  39% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###8       |  39% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###8       |  39% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###9       |  39% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###9       |  39% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###9       |  40% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###9       |  40% \n",
      "pytorch-1.10.0       | 950.8 MB  | ###9       |  40% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####       |  40% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####       |  40% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####       |  40% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####       |  41% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####       |  41% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####       |  41% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####1      |  41% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####1      |  41% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####1      |  41% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####1      |  42% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####1      |  42% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####1      |  42% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####1      |  42% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####2      |  42% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####2      |  42% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####2      |  42% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####2      |  42% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####2      |  43% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####2      |  43% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####2      |  43% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####3      |  43% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####3      |  43% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####3      |  43% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####3      |  43% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####3      |  44% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####3      |  44% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####3      |  44% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####3      |  44% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####4      |  44% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####4      |  44% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####4      |  44% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####4      |  45% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####4      |  45% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####4      |  45% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####4      |  45% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####5      |  45% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####5      |  45% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####5      |  45% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####5      |  45% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####5      |  46% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####5      |  46% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####5      |  46% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####5      |  46% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####6      |  46% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####6      |  46% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####6      |  46% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####6      |  47% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####6      |  47% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####6      |  47% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####6      |  47% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####7      |  47% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####7      |  47% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####7      |  47% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####7      |  47% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####7      |  48% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####7      |  48% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####7      |  48% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####7      |  48% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####8      |  48% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####8      |  48% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####8      |  48% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####8      |  48% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####8      |  49% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####8      |  49% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####8      |  49% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####8      |  49% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####9      |  49% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####9      |  49% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####9      |  49% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####9      |  50% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####9      |  50% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####9      |  50% \n",
      "pytorch-1.10.0       | 950.8 MB  | ####9      |  50% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####      |  50% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####      |  50% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####      |  50% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####      |  51% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####      |  51% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####      |  51% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####      |  51% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####1     |  51% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####1     |  51% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####1     |  51% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####1     |  51% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####1     |  52% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####1     |  52% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####1     |  52% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####2     |  52% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####2     |  52% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####2     |  52% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####2     |  52% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####2     |  53% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####2     |  53% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####2     |  53% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####3     |  53% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####3     |  53% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####3     |  53% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####3     |  53% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####3     |  54% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####3     |  54% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####3     |  54% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####4     |  54% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####4     |  54% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####4     |  54% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####4     |  54% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####4     |  55% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####4     |  55% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####4     |  55% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####5     |  55% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####5     |  55% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####5     |  55% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####5     |  55% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####5     |  56% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####5     |  56% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####5     |  56% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####6     |  56% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####6     |  56% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####6     |  56% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####6     |  56% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####6     |  57% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####6     |  57% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####6     |  57% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####6     |  57% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####7     |  57% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####7     |  57% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####7     |  57% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####7     |  58% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####7     |  58% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####7     |  58% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####8     |  58% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####8     |  58% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####8     |  58% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####8     |  58% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####8     |  59% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####8     |  59% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####9     |  59% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####9     |  59% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####9     |  59% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####9     |  59% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####9     |  60% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####9     |  60% \n",
      "pytorch-1.10.0       | 950.8 MB  | #####9     |  60% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######     |  60% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######     |  60% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######     |  60% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######     |  60% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######     |  61% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######     |  61% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######     |  61% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######     |  61% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######1    |  61% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######1    |  61% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######1    |  61% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######1    |  61% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######1    |  62% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######1    |  62% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######1    |  62% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######2    |  62% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######2    |  62% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######2    |  62% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######2    |  63% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######2    |  63% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######2    |  63% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######3    |  63% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######3    |  63% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######3    |  63% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######3    |  64% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######3    |  64% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######3    |  64% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######3    |  64% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######4    |  64% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######4    |  64% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######4    |  64% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######4    |  64% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######4    |  65% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######4    |  65% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######4    |  65% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######5    |  65% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######5    |  65% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######5    |  65% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######5    |  66% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######5    |  66% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######5    |  66% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######6    |  66% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######6    |  66% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######6    |  66% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######6    |  67% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######6    |  67% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######6    |  67% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######7    |  67% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######7    |  67% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######7    |  67% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######7    |  68% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######7    |  68% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######7    |  68% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######8    |  68% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######8    |  68% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######8    |  68% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######8    |  69% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######8    |  69% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######8    |  69% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######9    |  69% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######9    |  69% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######9    |  69% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######9    |  70% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######9    |  70% \n",
      "pytorch-1.10.0       | 950.8 MB  | ######9    |  70% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######    |  70% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######    |  70% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######    |  70% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######    |  71% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######    |  71% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######    |  71% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######    |  71% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######1   |  71% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######1   |  71% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######1   |  72% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######1   |  72% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######1   |  72% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######2   |  72% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######2   |  72% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######2   |  72% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######2   |  73% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######2   |  73% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######2   |  73% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######3   |  73% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######3   |  73% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######3   |  74% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######3   |  74% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######3   |  74% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######4   |  74% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######4   |  74% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######4   |  74% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######4   |  75% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######4   |  75% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######4   |  75% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######5   |  75% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######5   |  75% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######5   |  75% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######5   |  76% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######5   |  76% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######5   |  76% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######6   |  76% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######6   |  76% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######6   |  76% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######6   |  77% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######6   |  77% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######6   |  77% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######7   |  77% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######7   |  77% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######7   |  78% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######7   |  78% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######7   |  78% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######8   |  78% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######8   |  78% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######8   |  78% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######8   |  79% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######8   |  79% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######9   |  79% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######9   |  79% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######9   |  79% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######9   |  80% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######9   |  80% \n",
      "pytorch-1.10.0       | 950.8 MB  | #######9   |  80% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########   |  80% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########   |  80% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########   |  80% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########   |  81% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########   |  81% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########1  |  81% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########1  |  81% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########1  |  81% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########1  |  82% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########1  |  82% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########1  |  82% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########2  |  82% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########2  |  82% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########2  |  82% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########2  |  83% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########2  |  83% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########2  |  83% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########3  |  83% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########3  |  83% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########3  |  83% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########3  |  84% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########3  |  84% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########3  |  84% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########4  |  84% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########4  |  84% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########4  |  85% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########4  |  85% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########4  |  85% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########5  |  85% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########5  |  85% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########5  |  86% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########5  |  86% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########5  |  86% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########6  |  86% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########6  |  86% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########6  |  86% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########6  |  87% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########6  |  87% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########7  |  87% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########7  |  87% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########7  |  87% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########7  |  88% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########7  |  88% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########8  |  88% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########8  |  88% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########8  |  88% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########8  |  89% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########8  |  89% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########8  |  89% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########9  |  89% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########9  |  89% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########9  |  89% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########9  |  90% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########9  |  90% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########9  |  90% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########  |  90% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########  |  90% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########  |  90% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########  |  91% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########  |  91% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########  |  91% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########1 |  91% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########1 |  91% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########1 |  91% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########1 |  92% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########1 |  92% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########1 |  92% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########2 |  92% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########2 |  92% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########2 |  92% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########2 |  93% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########2 |  93% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########2 |  93% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########2 |  93% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########3 |  93% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########3 |  93% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########3 |  93% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########3 |  93% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########3 |  94% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########3 |  94% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########3 |  94% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########4 |  94% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########4 |  94% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########4 |  94% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########4 |  94% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########4 |  95% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########4 |  95% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########4 |  95% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########4 |  95% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########5 |  95% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########5 |  95% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########5 |  95% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########5 |  96% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########5 |  96% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########5 |  96% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########5 |  96% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########6 |  96% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########6 |  96% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########6 |  96% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########6 |  97% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########6 |  97% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########6 |  97% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########7 |  97% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########7 |  97% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########7 |  98% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########7 |  98% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########7 |  98% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########8 |  98% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########8 |  98% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########8 |  98% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########8 |  99% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########8 |  99% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########9 |  99% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########9 |  99% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########9 |  99% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########9 | 100% \n",
      "pytorch-1.10.0       | 950.8 MB  | #########9 | 100% \n",
      "pytorch-1.10.0       | 950.8 MB  | ########## | 100% \n",
      "\n",
      "cudatoolkit-10.2.89  | 317.2 MB  |            |   0% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  |            |   0% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 1          |   1% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 1          |   2% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 2          |   2% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 3          |   3% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 3          |   4% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 4          |   4% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 4          |   5% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 5          |   5% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 6          |   6% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 6          |   7% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 7          |   7% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 7          |   8% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 8          |   9% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 9          |   9% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | 9          |  10% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #          |  10% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #          |  11% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #1         |  12% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #2         |  12% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #2         |  13% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #3         |  13% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #3         |  14% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #4         |  15% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #5         |  15% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #5         |  16% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #6         |  16% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #7         |  17% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #7         |  18% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #8         |  18% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #8         |  19% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #9         |  19% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #9         |  20% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##         |  21% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##1        |  21% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##1        |  22% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##2        |  22% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##2        |  23% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##3        |  24% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##4        |  24% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##4        |  25% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##5        |  25% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##5        |  26% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##6        |  26% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##7        |  27% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##7        |  28% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##8        |  28% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##8        |  29% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ##9        |  29% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###        |  30% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###        |  31% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###1       |  31% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###1       |  32% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###2       |  33% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###3       |  33% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###3       |  34% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###4       |  34% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###4       |  35% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###5       |  35% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###6       |  36% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###6       |  37% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###7       |  37% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###7       |  38% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###8       |  38% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###9       |  39% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ###9       |  40% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####       |  40% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####       |  41% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####1      |  41% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####1      |  42% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####2      |  42% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####2      |  43% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####3      |  44% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####4      |  44% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####4      |  45% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####5      |  45% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####6      |  46% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####6      |  47% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####7      |  47% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####7      |  48% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####8      |  49% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####9      |  49% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ####9      |  50% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####      |  50% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####      |  51% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####1     |  52% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####2     |  52% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####2     |  53% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####3     |  53% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####3     |  54% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####4     |  54% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####5     |  55% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####5     |  56% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####6     |  56% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####6     |  57% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####7     |  57% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####7     |  58% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####8     |  58% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####9     |  59% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #####9     |  60% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######     |  60% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######     |  60% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######     |  61% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######1    |  61% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######2    |  62% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######2    |  63% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######3    |  63% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######3    |  64% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######4    |  64% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######5    |  65% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######5    |  66% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######6    |  66% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######6    |  67% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######7    |  67% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######7    |  68% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######8    |  68% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######8    |  69% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ######9    |  70% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######    |  70% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######    |  71% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######1   |  71% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######1   |  72% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######2   |  72% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######2   |  73% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######3   |  73% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######3   |  73% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######3   |  74% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######4   |  74% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######4   |  75% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######5   |  75% \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cudatoolkit-10.2.89  | 317.2 MB  | #######5   |  76% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######6   |  76% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######6   |  77% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######6   |  77% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######7   |  77% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######7   |  78% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######8   |  78% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######8   |  79% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######8   |  79% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######9   |  79% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #######9   |  80% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########   |  80% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########   |  81% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########1  |  81% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########1  |  82% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########2  |  82% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########2  |  83% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########3  |  83% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########3  |  84% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########4  |  85% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########5  |  85% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########5  |  86% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########6  |  86% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########6  |  87% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########7  |  87% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########7  |  88% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########8  |  88% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########9  |  89% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########9  |  90% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########  |  90% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########  |  91% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########1 |  91% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########1 |  92% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########2 |  93% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########3 |  93% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########3 |  94% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########4 |  94% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########4 |  95% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########5 |  95% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########6 |  96% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########6 |  97% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########7 |  97% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########8 |  98% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########8 |  99% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########9 |  99% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | #########9 | 100% \n",
      "cudatoolkit-10.2.89  | 317.2 MB  | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n"
     ]
    }
   ],
   "source": [
    "conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2558821a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
